Dataset: HAR
--------------------------------------------
Model Name: MVAE_Soft_Assignment_GMM_full_10_HAR
==================================================================
MVAE_Soft_Assignment_GMM_full_10_HAR Part:
==================================================================
Random Seed = 10
--------------------------------------------
Device = cuda:1
--------------------------------------------
Number of Epochs = 2000
--------------------------------------------
Learning Rate = 0.0001
--------------------------------------------
Step Size = 20
--------------------------------------------
Input Dimension = 561
--------------------------------------------
Encoder Hidden Dimension 1 = 500
--------------------------------------------
Encoder Hidden Dimension 2 = 500
--------------------------------------------
Encoder Hidden Dimension 3 = 2000
--------------------------------------------
Latent Dimension = 5
--------------------------------------------
Decoder Hidden Layer 1 = 2000
--------------------------------------------
Decoder Hidden Layer 2 = 500
--------------------------------------------
Decoder Hidden Layer 3 = 500
--------------------------------------------
Output Dimension = 561
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 100
--------------------------------------------
Batch Size (Test) = 100
--------------------------------------------
gamma = 0.9
==================================================================

GMM Part:
==================================================================
Number of Components= 6
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-20
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 2001
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[-527.5971, -530.3849, -519.1509, -519.3798, -520.0727, -520.0365],
        [-634.8154, -520.0609, -522.8230, -519.2642, -540.3925, -520.2043],
        [-535.0856, -526.3307, -538.0787, -526.5552, -536.5623, -536.5554],
        [-521.6447, -530.9950, -518.9751, -519.9119, -519.4689, -519.1721],
        [-605.0966, -525.2329, -526.3684, -531.3035, -532.8384, -532.0780],
        [-534.7209, -541.7313, -531.1992, -537.5493, -524.8859, -535.1777],
        [-557.6188, -523.9564, -521.0323, -529.9120, -552.2673, -521.1382],
        [-526.8585, -535.6567, -532.0671, -523.9355, -536.0690, -524.9352],
        [-522.6154, -528.3641, -520.1969, -520.0403, -521.6290, -520.3331],
        [-524.8733, -529.8754, -525.6224, -575.3468, -523.0463, -523.6820],
        [-528.4469, -530.8856, -523.4308, -520.6379, -521.0700, -519.8746],
        [-525.8007, -525.3235, -519.5328, -519.5464, -519.5782, -519.9020],
        [-523.4897, -523.5565, -534.6246, -524.0800, -522.7192, -523.0728],
        [-524.3042, -561.1927, -559.0825, -525.8948, -525.4810, -521.8451],
        [-522.1256, -528.4760, -520.4244, -519.3220, -523.4561, -522.0681],
        [-539.7291, -522.9962, -524.3141, -522.3939, -523.1615, -522.5898],
        [-523.1996, -530.1380, -520.2720, -520.4863, -520.9954, -520.5822],
        [-520.9241, -523.8861, -524.4879, -524.0286, -522.5801, -522.8297],
        [-521.8317, -534.2148, -534.5708, -547.4487, -527.0708, -526.6880],
        [-531.9584, -566.0754, -531.0710, -680.2185, -532.4753, -541.4752],
        [-518.7428, -542.9200, -545.2083, -547.8857, -546.2642, -545.4361],
        [-529.5536, -525.1875, -529.1522, -529.5743, -529.3168, -525.2654],
        [-523.6527, -526.7200, -534.9269, -535.3780, -527.7319, -528.4731],
        [-524.5071, -524.4128, -530.1439, -524.5645, -524.8187, -524.5311],
        [-522.7856, -533.9248, -524.0419, -528.6115, -521.4573, -527.6597],
        [-525.7636, -536.5205, -530.7365, -596.3837, -527.9877, -533.0986],
        [-586.1823, -523.5530, -522.0373, -522.7301, -521.5862, -521.3290],
        [-530.0106, -543.7015, -533.1542, -609.6084, -528.6080, -527.9421],
        [-592.3764, -527.8772, -518.5641, -528.0647, -518.3385, -518.3040],
        [-522.6606, -536.8375, -564.4088, -527.4386, -522.9476, -522.5391],
        [-524.1878, -529.2362, -524.6201, -528.6376, -523.6172, -525.5501],
        [-525.1667, -544.4749, -525.3440, -556.8632, -525.5909, -525.7980],
        [-521.7918, -527.7968, -521.5860, -523.3171, -523.7548, -523.0040],
        [-522.3093, -524.9988, -522.8715, -537.3024, -524.1045, -546.5253],
        [-531.4335, -532.3210, -527.6147, -523.5228, -542.1602, -523.0226],
        [-524.9327, -531.1236, -530.5438, -571.4120, -527.3326, -529.9657],
        [-525.4496, -527.1692, -535.6780, -530.0120, -522.8951, -526.6152],
        [-524.2969, -531.7637, -531.6196, -524.7521, -524.2140, -525.0935],
        [-525.4493, -526.3224, -524.4818, -552.4514, -524.2513, -525.0854],
        [-535.7025, -525.5648, -522.2319, -522.9065, -521.2151, -533.0889],
        [-519.1133, -534.9889, -518.3662, -519.8204, -522.3787, -518.7472],
        [-521.2069, -525.4855, -525.0689, -531.7126, -524.0969, -537.5934],
        [-568.2391, -531.1876, -520.4196, -528.1345, -520.9724, -522.9730],
        [-522.6141, -521.3517, -522.8391, -521.3956, -521.6536, -521.8311],
        [-585.2092, -529.7468, -528.9548, -522.4396, -526.6107, -521.0691],
        [-522.0188, -538.7923, -520.7231, -556.8862, -520.7354, -528.7656],
        [-535.8502, -525.9600, -537.3259, -536.2227, -535.1000, -538.8738],
        [-526.3554, -520.9497, -529.5743, -527.9752, -575.6500, -522.6174],
        [-592.1735, -532.6292, -530.4540, -520.5822, -522.7308, -522.8551],
        [-522.1172, -530.9033, -525.1016, -524.3594, -523.6134, -523.9744],
        [-531.9578, -519.4688, -529.1693, -519.1454, -529.7595, -529.1633],
        [-610.4459, -533.4748, -521.6553, -522.5717, -522.8391, -523.2027]],
       device='cuda:1', grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([52, 6, 5]), scale: torch.Size([52, 6, 5]))
=============================================================================================
Decoder Loss (Test) = tensor([[-526.3730, -527.4084, -528.2233, -526.5998, -526.1886, -526.6325],
        [-527.8133, -530.0111, -528.0295, -528.1406, -527.3252, -527.8853],
        [-526.8181, -529.8799, -527.2419, -528.1180, -525.9213, -527.6859],
        [-524.6526, -526.3973, -526.2495, -525.5095, -524.2010, -525.3393],
        [-522.7986, -524.8150, -524.9597, -523.9330, -522.3404, -523.7518],
        [-522.9222, -524.3072, -526.7205, -524.1255, -522.6708, -524.0469],
        [-524.2246, -525.9773, -526.7192, -525.3473, -523.9781, -525.2381],
        [-526.0660, -527.8591, -527.4412, -526.7574, -526.3377, -526.5503],
        [-521.5091, -523.3627, -530.5736, -524.4047, -523.3945, -524.3963],
        [-521.4717, -524.0247, -531.6941, -525.0586, -523.9128, -524.8944],
        [-521.2393, -524.4384, -533.6595, -526.0566, -524.3427, -526.0430],
        [-520.9334, -523.7230, -532.3008, -525.3030, -523.6946, -525.3951],
        [-520.0127, -522.8409, -531.3363, -524.1022, -522.6312, -524.2062],
        [-521.0422, -523.2858, -532.7451, -525.0248, -523.7736, -525.1442],
        [-520.2512, -522.8409, -531.5479, -524.4181, -523.0349, -524.4470],
        [-520.5303, -523.2572, -531.5140, -524.5856, -523.1206, -524.6326],
        [-521.9438, -524.4731, -531.2719, -525.6998, -523.8804, -525.6388],
        [-521.7416, -524.2355, -530.8149, -525.2683, -523.5172, -525.1802],
        [-521.6283, -524.2744, -532.1101, -525.6302, -523.9757, -525.6018],
        [-522.2964, -524.5522, -530.7574, -525.3257, -523.9653, -525.1513],
        [-521.6025, -523.8103, -530.1236, -524.5206, -523.2924, -524.4555],
        [-523.2997, -526.0086, -531.3174, -526.5465, -525.0371, -526.4590],
        [-522.7626, -525.1579, -530.0476, -525.9108, -524.3690, -525.7632],
        [-521.5392, -523.9730, -530.3785, -524.9067, -523.1683, -524.8884],
        [-521.6517, -524.0552, -530.3076, -524.9691, -523.3099, -524.8734],
        [-522.3620, -523.9824, -531.1091, -525.5408, -524.4879, -525.4272],
        [-521.3517, -523.3181, -531.0483, -524.7008, -523.5086, -524.6992],
        [-521.2250, -523.1168, -531.0647, -524.8226, -523.4316, -524.8304],
        [-524.3845, -525.4431, -526.1913, -524.9363, -523.6868, -524.9121],
        [-525.7737, -525.9764, -527.4363, -525.7296, -525.7207, -525.7893],
        [-523.9503, -524.7121, -525.2148, -524.2880, -523.6221, -524.2134],
        [-525.2083, -526.1632, -526.9999, -526.0042, -524.7853, -525.9185],
        [-523.5778, -525.0588, -525.1116, -524.5033, -523.3223, -524.3139],
        [-522.9086, -525.8553, -524.0468, -524.0937, -522.4647, -523.7498],
        [-521.9188, -524.3305, -523.8884, -523.2006, -521.2982, -523.0308],
        [-522.6367, -524.7037, -524.5782, -524.0106, -522.1302, -523.7378],
        [-525.4623, -526.6667, -527.4884, -525.9590, -525.1261, -525.8376],
        [-524.0242, -526.3275, -532.1111, -527.1207, -525.7661, -527.1118],
        [-522.2168, -524.0757, -529.6837, -524.9517, -523.6881, -524.8795],
        [-520.9847, -522.4099, -528.7673, -523.3627, -522.4280, -523.3605],
        [-520.3117, -523.1261, -530.1536, -523.9100, -522.5447, -523.8522],
        [-519.7660, -522.2910, -530.2059, -523.6925, -522.1580, -523.7812],
        [-521.6605, -523.8823, -530.5629, -524.9217, -523.4106, -524.9805],
        [-521.5294, -523.6453, -528.4357, -524.1957, -522.7799, -524.1379],
        [-522.0504, -523.3239, -528.8865, -524.4874, -523.3461, -524.4006],
        [-521.8755, -523.2295, -530.4653, -524.8722, -523.8763, -524.8801],
        [-521.2980, -522.4755, -528.4283, -523.8008, -523.0195, -523.7561]],
       device='cuda:1')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([47, 6, 5]), scale: torch.Size([47, 6, 5]))
=============================================================================================
