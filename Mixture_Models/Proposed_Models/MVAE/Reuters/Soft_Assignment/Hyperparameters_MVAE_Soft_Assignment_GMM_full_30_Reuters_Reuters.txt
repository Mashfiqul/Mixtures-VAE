Dataset: Reuters
--------------------------------------------
Model Name: MVAE_Soft_Assignment_GMM_full_30_Reuters
==================================================================
MVAE_Soft_Assignment_GMM_full_30_Reuters Part:
==================================================================
Random Seed = 30
--------------------------------------------
Device = cuda:1
--------------------------------------------
Number of Epochs = 2000
--------------------------------------------
Learning Rate = 0.0001
--------------------------------------------
Step Size = 20
--------------------------------------------
Input Dimension = 2000
--------------------------------------------
Encoder Hidden Dimension 1 = 500
--------------------------------------------
Encoder Hidden Dimension 2 = 500
--------------------------------------------
Encoder Hidden Dimension 3 = 2000
--------------------------------------------
Latent Dimension = 10
--------------------------------------------
Decoder Hidden Layer 1 = 2000
--------------------------------------------
Decoder Hidden Layer 2 = 500
--------------------------------------------
Decoder Hidden Layer 3 = 500
--------------------------------------------
Output Dimension = 2000
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 100
--------------------------------------------
Batch Size (Test) = 100
--------------------------------------------
gamma = 0.9
==================================================================

GMM Part:
==================================================================
Number of Components= 4
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-20
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 2001
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[-2515.8799, -2518.3438, -2662.5527, -2500.7300],
        [-2466.3494, -2327.1704, -2404.9717, -2358.5386],
        [-2539.8232, -2891.7517, -2735.3125, -2538.3174],
        [-2662.8848, -2574.8386, -2590.1265, -2604.3086],
        [-2642.8286, -2688.3613, -2628.5830, -2640.9866],
        [-2640.4983, -2787.3755, -2641.3157, -2635.8665],
        [-2655.1682, -2781.4014, -2763.1287, -2659.1611],
        [-2704.2876, -2824.3047, -2730.7202, -2705.7937],
        [-2572.6946, -2531.2305, -2570.1038, -2557.6782],
        [-2732.4199, -2834.5737, -2772.3000, -2730.9092],
        [-2675.3242, -2688.7661, -2656.3252, -2667.4717],
        [-2426.9790, -2858.5156, -2552.0359, -2387.5078],
        [-2577.4111, -1907.4535, -2604.2117, -2163.5520],
        [-2501.8462, -2506.9990, -2500.7820, -2477.9878],
        [-2658.9048, -2735.2771, -2659.9678, -2652.5542],
        [-2571.2366, -2464.6919, -2268.3015, -2451.6362],
        [-2316.0859, -2751.5518, -2336.1909, -2291.0361],
        [-2613.4854, -2966.3760, -2834.8696, -2607.2476],
        [-2752.7969, -2658.7583, -2778.8572, -2721.8870],
        [-2624.8313, -2844.7815, -2585.5574, -2586.2944],
        [-2423.7239, -2674.7861, -2371.8081, -2358.5942],
        [-2766.4043, -2835.5054, -2720.5049, -2771.4009],
        [-2712.7949, -2853.7324, -2785.7930, -2692.3555],
        [-2474.8628, -2390.8379, -2139.5618, -2348.0796],
        [-2435.0332, -2717.4297, -2889.5103, -2330.9724],
        [-2814.2007, -2082.8726, -2801.5869, -2911.5898],
        [-2679.7622, -2214.8787, -2776.7271, -2784.2446],
        [-2658.9927, -2703.9873, -2647.3403, -2665.2451],
        [-2488.8760, -2861.6897, -2578.5220, -2508.1819],
        [-2702.9917, -2685.6509, -2679.2764, -2700.8838],
        [-2539.9697, -2581.1736, -2375.7585, -2446.9985],
        [-2767.1616, -2685.5532, -2614.7551, -2774.1724],
        [-2542.3047, -2529.1064, -2307.0740, -2454.6333],
        [-2585.8584, -2488.8733, -2622.0518, -2561.8154],
        [-2826.2471, -2729.3882, -2837.9468, -2845.2522],
        [-2450.1895, -2515.1067, -2519.3594, -2446.1013],
        [-2641.6909, -2757.2607, -2638.3701, -2645.8037],
        [-2523.9414, -2682.4954, -2555.9019, -2530.6592],
        [-2521.6245, -2146.9319, -2758.4414, -2572.0586],
        [-2649.8887, -2589.5571, -2742.0996, -2635.6582],
        [-2345.2791, -3037.1611, -2936.6646, -2275.1099],
        [-2609.1987, -2723.6021, -2651.4714, -2606.3799],
        [-2567.1738, -2478.4524, -2420.9531, -2444.8208],
        [-2833.0017, -2801.8169, -2812.2302, -2799.2715],
        [-2637.4631, -2701.0154, -2599.7710, -2599.1094],
        [-2664.1309, -2690.9722, -2576.3799, -2618.9675],
        [-2568.5515, -2536.1733, -2557.5586, -2515.4404],
        [-2732.7710, -2555.2480, -2623.0342, -2646.1245],
        [-2550.8521, -2098.0330, -2559.5305, -2542.7378],
        [-2769.7524, -2818.7261, -2812.0024, -2772.0864],
        [-2642.3435, -2740.1812, -2690.1929, -2645.8862],
        [-2752.7671, -2773.2271, -2743.4883, -2746.7017],
        [-2608.7642, -2827.3933, -2715.6296, -2593.7983],
        [-2511.7681, -2679.0244, -2456.2085, -2475.5669],
        [-2529.7227, -2774.9561, -2698.9871, -2542.6079],
        [-2538.8806, -3019.9153, -2833.9121, -2307.5535],
        [-2510.2805, -2607.0305, -2503.4429, -2503.2708],
        [-2840.5684, -2189.4392, -2827.3464, -2842.6519],
        [-2948.1089, -1897.4297, -2936.4492, -3008.4812],
        [-2554.9612, -2778.8740, -2649.3247, -2568.6929],
        [-2668.2480, -2823.9299, -2662.8706, -2639.2734],
        [-2667.8730, -2755.2749, -2705.0769, -2669.5728],
        [-2601.4668, -2159.4780, -2728.4902, -2299.9221],
        [-2569.0498, -2707.2615, -2583.2334, -2573.8198],
        [-2630.2166, -2703.2136, -2648.3635, -2624.8396],
        [-2342.5786, -3052.2803, -2872.5752, -2307.1819],
        [-2686.4253, -2834.5078, -2766.9165, -2699.0051],
        [-2350.6929, -2961.5337, -2730.2363, -2347.5403],
        [-2818.9990, -2715.6025, -2690.1304, -2712.7039],
        [-2609.6599, -2609.0950, -2563.9468, -2581.8374],
        [-2571.5132, -2473.8296, -2702.5791, -2557.6079],
        [-2627.5972, -2727.4993, -2643.9424, -2623.5100],
        [-2573.7405, -2621.0042, -2645.1138, -2567.7585],
        [-2350.1091, -2162.3120, -2379.9470, -2317.1250],
        [-2467.1826, -2519.5693, -2459.0767, -2462.8035],
        [-2653.4766, -2891.1543, -2745.7075, -2643.7712],
        [-2570.0312, -2395.4563, -2279.3547, -2501.9695],
        [-2664.9424, -2605.0791, -2650.1519, -2627.7729],
        [-2392.4521, -2940.3369, -2623.7812, -2366.1943],
        [-2672.1182, -2690.8577, -2652.1653, -2657.2832],
        [-2555.8154, -2548.6365, -2623.1128, -2550.6548],
        [-2676.3218, -2458.7715, -2709.8989, -2665.2471],
        [-2593.9219, -2961.0591, -2823.0669, -2587.9939],
        [-2780.4292, -2789.1147, -2787.2974, -2794.8870],
        [-2557.8086, -2893.5359, -2721.4465, -2561.5679],
        [-2806.6494, -2822.6550, -2802.9939, -2801.9905],
        [-2606.7737, -2789.7544, -2649.0947, -2605.9429],
        [-2622.0801, -2755.4714, -2631.1450, -2624.9058],
        [-2370.6895, -2635.0273, -2619.7769, -2371.5388],
        [-2528.1729, -2593.4390, -2522.6624, -2521.3418],
        [-2631.0830, -2875.6689, -2711.9692, -2632.8335],
        [-2330.6985, -2871.8149, -2686.5830, -2320.2246],
        [-2752.5547, -2778.4023, -2740.4497, -2739.9639],
        [-2489.2158, -2896.7158, -2928.4897, -2331.0029],
        [-2648.9106, -2435.4690, -2687.2637, -2637.5823],
        [-2529.2627, -2392.4270, -2374.4971, -2420.9492],
        [-2477.7300, -3166.7451, -2793.1536, -2443.9087],
        [-2527.4822, -2529.9111, -2513.2539, -2519.2253],
        [-2480.9170, -2911.9297, -2680.4683, -2480.5552],
        [-2625.0122, -2668.6270, -2617.2261, -2622.5703]], device='cuda:1',
       grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([100, 4, 10]), scale: torch.Size([100, 4, 10]))
=============================================================================================
Decoder Loss (Test) = tensor([[-2796.7349, -2811.2070, -2787.4651, -2798.8145],
        [-2639.6011, -2635.3870, -2626.9531, -2644.8506],
        [-2794.3132, -2325.6921, -2710.0850, -2853.7749],
        [-2545.5837, -2140.9678, -2455.3906, -2472.4712],
        [-2753.7178, -2790.9829, -2754.0425, -2752.8367],
        [-2732.9541, -2766.3569, -2762.5173, -2731.7129],
        [-2511.7881, -2536.8394, -2509.2827, -2511.9878],
        [-2567.1187, -2973.0652, -2662.2698, -2577.4688],
        [-2565.1418, -2622.5225, -2598.6646, -2516.4443],
        [-2578.6426, -2423.7324, -2490.5430, -2525.2290],
        [-2563.3286, -2876.8723, -2650.2773, -2566.8899],
        [-2537.5439, -2807.6074, -2698.4839, -2500.9727],
        [-2789.9224, -2822.6265, -2790.8467, -2791.2358],
        [-2398.1309, -2208.6553, -1945.6040, -2260.1997],
        [-2704.3521, -2760.4443, -2717.3999, -2712.5127],
        [-2607.9194, -2670.0889, -2699.7446, -2611.3984],
        [-2744.1619, -2727.0068, -2754.9419, -2741.0117],
        [-2800.2534, -2771.2275, -2785.2588, -2794.4834],
        [-2560.5278, -2960.2129, -2831.2034, -2553.1089],
        [-2723.5181, -2678.3389, -2739.9541, -2724.0857],
        [-2539.4316, -2483.7139, -2404.1560, -2429.0898],
        [-2449.5913, -2018.6818, -2650.2983, -2151.4143],
        [-2604.5193, -2632.3438, -2613.7041, -2588.6831],
        [-2651.3015, -2588.5923, -2548.1030, -2571.2231],
        [-2449.9937, -2613.5081, -2549.9998, -2424.7808],
        [-2676.6416, -2235.9573, -2730.1865, -2755.4536],
        [-2524.5452, -2887.6472, -2700.5093, -2524.1616],
        [-2650.1484, -2629.6873, -2642.4429, -2668.2844],
        [-2560.6992, -2277.2812, -2657.6687, -2446.1028],
        [-2639.0015, -2701.4102, -2666.5691, -2625.5088],
        [-2706.4946, -2830.1982, -2769.0142, -2713.8965],
        [-2690.2490, -2682.1245, -2593.3948, -2628.9927],
        [-2508.6367, -2530.9980, -2346.3132, -2410.0051],
        [-2673.4370, -2682.5088, -2639.1255, -2653.1309],
        [-2730.7593, -2521.6414, -2753.9121, -2598.1821],
        [-2726.8037, -2810.4182, -2762.1550, -2727.2866],
        [-2681.1479, -2654.0098, -2543.7109, -2612.3928],
        [-2706.3188, -2784.9841, -2721.0464, -2719.4663],
        [-2657.7996, -2822.2808, -2700.6094, -2659.2424],
        [-2608.3682, -2782.1501, -2605.7485, -2607.5798],
        [-2475.0215, -2856.8369, -2858.7578, -2475.3994],
        [-2498.6487, -2760.1055, -2770.3279, -2457.3457],
        [-2646.7031, -2673.8931, -2635.4126, -2644.7083],
        [-2738.5496, -2754.5688, -2778.2065, -2733.7043],
        [-2598.5029, -2537.5435, -2563.7676, -2603.1318],
        [-2724.5576, -2663.0317, -2580.4363, -2653.1934],
        [-2660.1025, -2649.2310, -2619.0601, -2639.6216],
        [-2627.7710, -2601.7231, -2493.6460, -2594.2705],
        [-2537.6399, -2371.7888, -2271.3079, -2332.5566],
        [-2686.8909, -2759.3862, -2679.7417, -2690.9189],
        [-2459.4260, -2726.6509, -2461.3223, -2447.7759],
        [-2668.8491, -2591.8564, -2570.2847, -2572.5376],
        [-2640.4751, -2785.1631, -2695.7332, -2651.1377],
        [-2776.6919, -2682.7429, -2785.4304, -2733.7512],
        [-2659.4697, -2708.7725, -2687.3469, -2663.2744],
        [-2763.8975, -2800.7520, -2775.3484, -2763.1401],
        [-2603.7051, -2608.7654, -2734.9258, -2593.6519],
        [-2195.6040, -2670.9546, -2542.6428, -2117.3872],
        [-2467.4634, -2894.4375, -2908.4585, -2399.6147],
        [-2575.7092, -2829.2654, -2636.1245, -2570.4941],
        [-2587.7412, -2850.1270, -2581.0195, -2584.2183],
        [-2716.8679, -2837.9927, -2801.1597, -2718.7832],
        [-2609.6660, -2784.0015, -2694.3052, -2622.5979],
        [-2612.3276, -2685.6870, -2719.6528, -2627.0684],
        [-2707.9946, -2829.3206, -2771.8711, -2724.5830],
        [-2277.6614, -2661.5791, -2933.7913, -2261.3984],
        [-2707.8433, -2785.5430, -2711.0811, -2713.6104],
        [-2641.3423, -2741.9373, -2630.0325, -2634.2856],
        [-2494.0298, -2858.0991, -2590.1919, -2484.6938],
        [-2791.3413, -2805.9133, -2759.1824, -2775.1416],
        [-2559.0391, -2516.6804, -2553.2473, -2478.6401],
        [-2673.3057, -2670.7668, -2677.2524, -2677.0366],
        [-2615.5713, -2831.2085, -2611.0044, -2608.7051],
        [-2552.6279, -2794.2900, -2660.0310, -2562.5942],
        [-2686.0403, -2857.8967, -2797.6628, -2637.7351],
        [-2830.5940, -2611.9756, -2429.0107, -2819.1362],
        [-2652.3879, -2661.9788, -2709.9536, -2655.0415],
        [-2575.5527, -2856.6665, -2571.4260, -2573.9138],
        [-2571.6211, -2675.3997, -2594.2710, -2544.1033],
        [-2751.1631, -2756.0139, -2747.0405, -2754.5239],
        [-2399.4055, -2904.7019, -2412.4399, -2292.3701],
        [-2551.4126, -2473.8945, -2383.4927, -2410.2310],
        [-2800.6804, -2703.1169, -2800.3394, -2795.7671],
        [-2702.0647, -2932.8643, -2812.5845, -2628.2842],
        [-2609.5835, -2797.9602, -2604.3389, -2615.6792],
        [-2753.7192, -2776.4385, -2760.2329, -2756.5010],
        [-2456.9636, -2715.5522, -2667.1553, -2455.0413],
        [-2607.8931, -2040.2280, -2537.2329, -2533.1060],
        [-2743.9746, -2865.8271, -2764.7576, -2743.0928],
        [-2261.2329, -2156.5840, -2150.4536, -2197.8525],
        [-2402.1040, -2730.8079, -2841.5864, -2269.7183],
        [-2761.5652, -2790.8691, -2768.9858, -2756.9458],
        [-2563.5408, -2688.8491, -2724.4697, -2585.5781],
        [-2892.4976, -2383.6025, -2905.4814, -2977.7617],
        [-2412.0405, -2710.5344, -2900.3892, -2334.9741],
        [-2593.8379, -2822.4302, -2693.4971, -2594.5098],
        [-2558.1506, -2869.5127, -2667.3926, -2559.1021],
        [-2477.4358, -2761.8245, -2862.1299, -2383.1106],
        [-2732.9729, -2693.0427, -2664.3306, -2709.3936],
        [-2638.6421, -2645.6943, -2640.8779, -2658.7612]], device='cuda:1')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([100, 4, 10]), scale: torch.Size([100, 4, 10]))
=============================================================================================
