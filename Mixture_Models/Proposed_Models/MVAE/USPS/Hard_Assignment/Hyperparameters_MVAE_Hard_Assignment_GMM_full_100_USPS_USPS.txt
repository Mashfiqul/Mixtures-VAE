Dataset: USPS
--------------------------------------------
Model Name: MVAE_Hard_Assignment_GMM_full_100_USPS
==================================================================
MVAE_Hard_Assignment_GMM_full_100_USPS Part:
==================================================================
Random Seed = 100
--------------------------------------------
Device = cuda:1
--------------------------------------------
Number of Epochs = 2000
--------------------------------------------
Learning Rate = 0.0001
--------------------------------------------
Step Size = 20
--------------------------------------------
Input Dimension = 256
--------------------------------------------
Encoder Hidden Dimension 1 = 500
--------------------------------------------
Encoder Hidden Dimension 2 = 500
--------------------------------------------
Encoder Hidden Dimension 3 = 2000
--------------------------------------------
Latent Dimension = 10
--------------------------------------------
Decoder Hidden Layer 1 = 2000
--------------------------------------------
Decoder Hidden Layer 2 = 500
--------------------------------------------
Decoder Hidden Layer 3 = 500
--------------------------------------------
Output Dimension = 256
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 100
--------------------------------------------
Batch Size (Test) = 100
--------------------------------------------
gamma = 0.9
==================================================================

GMM Part:
==================================================================
Number of Components= 10
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-20
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 2001
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[-103.0388, -126.0787, -131.8007, -102.2366, -110.8075, -104.1519,
          -98.2398, -115.6976, -128.3858, -120.4673],
        [-137.6736, -108.6655, -105.7416, -127.9485, -112.3880, -106.9172,
         -119.2051, -104.4634, -399.4391, -155.0887],
        [ -52.5961,  -55.9518,  -66.5094,  -58.4195,  -63.7116,  -67.3483,
          -81.3662,  -53.3868,  -73.3170,  -50.5040],
        [ -96.5039, -122.0732, -141.7745, -107.5721,  -94.2178, -110.0672,
         -114.6210, -100.2954, -133.5479,  -91.9235],
        [ -67.0129,  -69.7355,  -91.0889,  -75.1531,  -79.0208,  -77.4265,
          -69.3421,  -70.4606, -138.6650,  -71.9542],
        [ -47.8887,  -46.5679,  -66.7809,  -51.0274,  -43.4418,  -40.8413,
          -59.3373,  -47.4762,  -41.3511,  -54.1396],
        [ -75.2960,  -76.3694,  -92.8227,  -80.8714,  -83.7087, -105.2464,
          -89.6832,  -75.6531, -138.4193,  -82.6480],
        [ -57.8204,  -46.9495,  -54.6480,  -52.7243,  -58.5211,  -51.5976,
          -59.6203,  -63.3504,  -47.9520,  -57.2893],
        [ -92.6963,  -92.5709, -112.3774, -124.8808, -112.5632,  -95.9358,
         -112.8979,  -96.6730, -235.0794,  -89.2932],
        [-154.4236, -132.6923, -158.6708, -114.6597, -132.4337, -130.3351,
         -119.8874, -137.4522, -224.2163, -139.4380],
        [-107.4421, -118.5301, -120.9086,  -96.0358, -103.7313, -126.7354,
         -101.4649, -104.1697, -141.8499, -108.4097],
        [ -54.6072,  -45.0211,  -62.6310,  -46.3489,  -45.6589,  -47.7721,
          -47.9428,  -46.3930,  -44.8199,  -56.7343],
        [-117.4663, -116.2693, -129.1647,  -96.4883, -108.9665, -120.7750,
         -109.7350, -105.4493, -136.6174, -100.4978],
        [-121.9674, -107.9966, -121.6642,  -95.8134,  -87.1071,  -92.8206,
         -101.9252, -116.0210,  -90.7310,  -95.3835],
        [ -84.7129,  -82.8546, -110.8490,  -93.2021,  -95.2325,  -85.4647,
          -86.9919,  -90.7083, -164.3282,  -88.8426],
        [ -65.4483,  -67.4639,  -87.6035,  -69.3675,  -71.1649,  -73.6270,
          -81.3811,  -68.1067, -132.4375,  -68.5980],
        [ -99.8941, -131.3048, -130.0861,  -96.3962, -105.7991, -101.7569,
         -102.7825,  -96.3999, -162.5970, -105.8489],
        [ -81.9437,  -70.4532,  -90.2825,  -75.7509,  -74.3414,  -69.2898,
          -69.1652,  -67.8207, -187.8376,  -89.0427],
        [ -92.2717,  -97.0952, -107.7201,  -98.7835,  -98.4817, -106.7957,
         -110.2787, -101.9281, -174.1858, -114.5678],
        [-119.3756, -127.0069, -145.7547, -103.1009,  -99.1465, -123.3211,
         -109.3596, -116.4339, -132.6152, -107.4386],
        [ -71.6833,  -72.0318,  -75.2397,  -69.5755,  -85.8002,  -88.3463,
          -75.9711,  -77.1159,  -78.7459,  -69.7758],
        [ -85.9135,  -86.0982, -101.0839,  -81.2709,  -90.2663,  -91.8053,
          -81.4151, -118.5157, -213.6938,  -97.6017],
        [ -75.6466,  -75.2304,  -92.2854,  -81.9158,  -83.8048, -104.0349,
          -83.4865,  -82.3368, -159.0125,  -87.4478],
        [ -84.1548,  -84.7286, -102.1732,  -77.8384,  -85.1173,  -97.5585,
          -82.1288,  -86.3416,  -89.4151,  -79.4457],
        [ -82.8242,  -82.3833, -103.8947,  -83.9193,  -75.0043,  -78.2180,
          -72.0763,  -73.3510, -249.4998,  -93.1178],
        [ -50.0175,  -51.3881,  -69.6836,  -49.0179,  -39.8938,  -42.6753,
          -57.9320,  -42.3249,  -39.0494,  -40.6808],
        [-102.1340,  -88.7205, -121.9124, -101.4017,  -86.8551,  -88.1260,
          -86.6096,  -75.1406, -297.7763, -107.6635],
        [ -77.9342,  -68.5522,  -78.6308,  -70.1346,  -73.1591,  -86.1011,
          -66.7617,  -64.0715,  -57.8623,  -68.6083],
        [-116.6619, -146.7765, -114.8588, -126.4688, -116.6777, -114.2637,
         -125.3494, -120.6205, -206.1223, -144.4425],
        [ -66.0399,  -64.6495,  -91.2523,  -73.5950,  -68.6380,  -74.1157,
          -79.2131,  -69.6786, -139.6019,  -72.7458],
        [-115.2155, -109.0606,  -90.8542,  -90.9335,  -85.2976,  -92.0010,
          -96.3234, -109.7339, -155.6761, -126.6367],
        [ -75.0424,  -69.3033, -125.6807,  -84.3308,  -81.7596,  -75.1117,
          -81.6726,  -78.2283, -223.0882,  -81.3600],
        [-105.0626, -104.1783, -111.7566,  -92.1098,  -91.2207, -112.3194,
          -92.8816, -102.0191, -118.1101, -114.7574],
        [-137.4518,  -90.6281, -123.1111,  -83.3065,  -84.3201, -106.1173,
          -94.7971,  -86.9995, -104.1731,  -93.5339],
        [ -74.6428,  -80.5650,  -90.5392,  -84.1019,  -84.5221,  -84.5678,
          -91.4335,  -84.5278, -148.7354,  -91.6784],
        [-105.3876, -100.5002,  -96.6320,  -99.4324, -103.6341, -101.1061,
         -104.7022, -103.1709, -141.4105, -115.0614],
        [ -80.8691,  -82.8840, -104.6958,  -92.4147,  -88.3802,  -92.0317,
          -84.3229,  -84.8431, -216.7176,  -84.1125],
        [-111.8807, -102.6234,  -93.9414, -101.2801,  -96.2344,  -98.8925,
         -108.9268,  -98.5258, -200.9492, -125.8856],
        [-110.2899, -107.7800,  -99.0659,  -92.4164,  -94.9069,  -96.8923,
         -100.4474, -106.2321, -108.5178, -110.6292],
        [ -69.9787,  -73.6191,  -85.5607,  -83.7287,  -77.6238,  -72.7034,
          -97.5283,  -72.9639, -120.8215,  -86.8123],
        [-124.7405, -119.7446, -116.2437, -123.8076, -106.4240, -108.1043,
         -115.2802, -109.3939, -236.7187, -114.1784],
        [-138.4987, -108.1545, -107.9979,  -98.2951, -101.7339,  -95.1297,
         -110.4027, -104.1530, -143.9552, -124.5851],
        [-136.1172, -118.4772, -123.6042, -136.3512, -128.9535, -115.7999,
         -116.0115, -118.1656, -262.3428, -153.0251],
        [ -86.1356,  -73.3193,  -87.1398,  -76.8510,  -69.9996,  -78.5869,
          -91.0001,  -69.9609, -284.5683, -111.9409],
        [ -97.4688,  -98.9196,  -85.4322,  -87.2755,  -96.5419, -106.2916,
          -90.7531, -101.5947, -169.7323,  -93.1236],
        [-115.2542, -108.2031, -108.1466, -115.5317, -113.9393, -106.6604,
         -107.1059, -105.0509, -148.8958, -126.8799],
        [ -87.1450, -111.5286, -106.4270,  -88.3786, -123.8438, -116.4303,
         -105.5785, -105.3659, -140.9114,  -81.3055],
        [-103.0685,  -89.4152, -137.2744,  -95.3111, -102.9955,  -98.9824,
          -93.0333,  -94.3369, -380.0338, -109.9435],
        [-109.9083, -106.6407, -139.7139, -115.7895, -109.0190, -114.1734,
         -103.2673, -113.8062, -194.5818, -115.6521],
        [ -82.7410,  -82.9934,  -90.7317, -102.0398,  -90.9429,  -88.0004,
          -91.9399, -112.7732,  -81.1561,  -72.4288],
        [ -86.6725, -114.6400, -113.6035, -118.7337, -120.7142, -131.2064,
         -108.4496, -104.7585, -115.5934,  -82.0017],
        [ -81.2299,  -67.4970,  -98.8549,  -66.8482,  -78.4801,  -99.6406,
          -71.7925,  -62.6984, -104.6601, -111.0947],
        [ -89.7443,  -64.4984, -109.4576,  -74.7092,  -64.3893,  -75.2319,
          -70.8264,  -61.6165, -145.9619, -131.8854],
        [ -90.4120,  -83.7293, -138.8081, -100.2937,  -99.3222, -114.9806,
          -86.2494,  -87.8889, -210.7274,  -97.9557],
        [-114.8258, -112.5032, -127.0536, -103.0462,  -98.8556, -114.6911,
         -111.8765, -106.6048, -152.5786, -119.6781],
        [ -87.3450,  -83.4947,  -82.7198,  -82.4227,  -71.9294, -102.5264,
          -84.5084, -101.9209, -108.6494,  -81.0746],
        [ -92.9396,  -74.3729,  -79.6773,  -89.6264,  -85.2946,  -99.1408,
          -86.8040,  -79.0349, -125.1748,  -86.0187],
        [ -88.4026,  -88.7007, -110.4417,  -88.9906,  -86.5590,  -98.0840,
          -96.7880,  -84.1363, -131.3637, -100.1331],
        [ -96.1035,  -85.5963, -114.2436, -101.8575,  -90.6964,  -93.2059,
          -93.8547,  -87.6293, -136.1446,  -96.8173],
        [-103.1855, -138.2992, -114.8254, -104.3490,  -95.5470, -111.9963,
         -119.3944, -108.6360, -129.3672, -101.7299],
        [ -98.1757, -113.3102, -113.5757, -100.3495, -100.1640, -102.2845,
          -95.0861,  -96.5465, -222.2370, -116.0930],
        [ -91.5044, -123.3653, -110.2616, -100.4937,  -95.3065, -123.9324,
          -92.8648, -109.1093, -128.5064, -100.6435],
        [-113.2518, -105.7342,  -82.6165,  -90.9252,  -94.7622,  -88.3924,
          -94.5154,  -88.3737, -189.7288,  -90.4267],
        [ -49.3244,  -53.6540,  -71.0995,  -47.0501,  -47.1935,  -46.4909,
          -67.6634,  -49.5879,  -43.8844,  -52.7609],
        [ -99.9040, -130.0709,  -99.6252, -101.8964, -118.3599, -101.0932,
         -107.3287, -116.2419, -116.3060, -113.1021],
        [-120.8588, -134.1908, -138.3965, -140.6540, -131.6389, -123.0860,
         -139.0540, -120.8251, -175.0757, -103.0964],
        [-118.9057, -136.2541, -196.9080, -129.5118, -105.8588, -122.0269,
         -118.5749, -108.7913, -162.9518, -133.3385],
        [ -87.1565,  -80.5490, -103.6690,  -80.0249,  -81.6802,  -92.0513,
          -91.1273,  -77.7987, -139.1810,  -85.9936],
        [-128.9515, -140.6430, -139.0564, -115.9739, -122.0925, -131.9335,
         -132.2556, -145.4397, -153.1286, -125.8099],
        [ -97.0265,  -94.8268, -124.5615, -105.9695, -104.0370,  -96.1681,
         -110.6711,  -94.4174, -182.4659, -107.3493],
        [ -77.6221,  -77.4608,  -82.3816,  -73.4976,  -79.7113,  -72.6008,
          -92.2883,  -80.6078, -131.1395,  -68.1079],
        [ -75.5679,  -88.5322,  -90.1406,  -85.4473,  -89.7255,  -90.9447,
          -89.9674,  -92.2464, -108.0544,  -75.7558],
        [-112.8819, -119.0531, -111.8384, -109.7098, -111.2804, -109.8993,
         -121.0127, -123.5311, -232.0888,  -93.2479],
        [ -61.7465,  -60.2608,  -68.5349,  -59.5412,  -70.5565,  -67.9581,
          -65.5861,  -56.0896,  -92.8971,  -71.1417],
        [ -68.5866,  -65.2631,  -86.8738,  -69.7045,  -75.7814,  -77.5942,
          -70.9681,  -74.3474, -106.0508,  -78.7244],
        [ -83.7754,  -99.5017, -106.1024,  -82.2280, -101.4916, -104.5280,
          -92.3559,  -87.7229, -134.0054,  -96.6755],
        [-115.0617, -129.3451, -131.3812, -118.3898, -111.3056, -122.7137,
         -117.0456, -114.3721, -270.4247, -151.0215],
        [ -70.0364,  -69.7926,  -88.5018,  -71.4515,  -82.5190,  -83.7292,
          -82.6271,  -75.3589, -100.7938,  -92.9052],
        [ -41.5110,  -46.1533,  -52.3100,  -43.0244,  -61.7238,  -52.7989,
          -53.1871,  -43.8054,  -44.0383,  -54.7356],
        [ -73.7647,  -85.5445,  -93.9476,  -77.8410,  -82.4977,  -93.6215,
          -86.4389,  -81.9488, -141.6858,  -91.9733],
        [ -82.7047,  -78.7176, -104.9396,  -97.3346,  -83.2121,  -92.9308,
          -84.8808,  -78.3458, -258.0789,  -88.6157],
        [ -77.5033,  -96.8237,  -89.4463,  -84.2675,  -96.0643,  -81.2853,
          -94.4476,  -90.9733, -115.2377,  -71.6200],
        [-104.5376,  -87.6783, -155.8557,  -85.8638,  -96.5215,  -92.6776,
          -84.2986,  -82.0853, -183.0249, -137.6304],
        [-118.0051,  -85.9033, -133.1457, -114.8616,  -87.5548,  -96.5653,
          -85.2474,  -81.0900, -322.6639, -115.0411],
        [-149.0188, -106.8425, -101.8443, -106.6261, -107.1198, -111.2056,
         -108.1913, -109.3938, -200.9838, -154.5502],
        [-103.4143, -137.9609, -116.9059, -100.9537, -111.7753,  -96.0065,
          -96.9868, -109.1668, -125.5744, -120.9439],
        [ -99.5213,  -82.5200, -100.1978,  -81.4490,  -87.1090,  -90.0896,
          -93.0242,  -83.6320, -112.6711,  -84.9170],
        [ -60.0160,  -67.9872,  -51.5990,  -64.2469,  -54.6923,  -76.1665,
          -73.3261,  -51.1750,  -50.1690,  -57.0227],
        [-144.7365, -129.4990, -118.8915, -121.3335, -107.7755, -121.5123,
         -106.6018, -110.8519, -263.6186, -125.4328],
        [ -97.8125, -103.7360, -130.9734,  -96.5888,  -83.5533, -116.9801,
          -80.5263,  -97.0973, -217.4767,  -92.4298],
        [ -82.0714,  -85.7007, -111.3840,  -96.6376, -102.7889,  -98.1082,
          -92.8912,  -86.5553, -275.5566, -142.0425]], device='cuda:1',
       grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([91, 10, 10]), scale: torch.Size([91, 10, 10]))
=============================================================================================
Decoder Loss (Test) = tensor([[-125.3940, -115.4740, -112.3315, -116.6339, -107.7971, -103.3233,
         -108.7368, -105.6712, -233.9401, -120.2257],
        [ -55.3192,  -54.7685,  -76.9354,  -59.5438,  -60.2269,  -62.6295,
          -61.4139,  -57.5516, -110.1192,  -66.4939],
        [ -84.2371,  -85.2460,  -86.3053,  -74.6485,  -74.8392,  -83.4032,
          -79.1552,  -77.3984, -114.1931,  -86.9115],
        [ -68.3173,  -69.7795,  -87.8303,  -71.5269,  -77.4825,  -80.1743,
          -73.9991,  -71.5007, -124.8480,  -80.1232],
        [ -94.0256,  -93.9698, -121.8478, -100.2780,  -94.2309, -100.1641,
         -107.2953,  -94.3937, -173.4177,  -98.8605],
        [-124.6277, -105.2400,  -94.9459,  -99.4760,  -97.5626,  -99.4319,
         -112.4611,  -96.2699, -105.0386, -115.3589],
        [ -56.6090,  -49.2505,  -61.0387,  -57.3713,  -55.4319,  -53.0255,
          -57.7268,  -48.7147,  -47.9951,  -55.5653]], device='cuda:1')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([7, 10, 10]), scale: torch.Size([7, 10, 10]))
=============================================================================================
