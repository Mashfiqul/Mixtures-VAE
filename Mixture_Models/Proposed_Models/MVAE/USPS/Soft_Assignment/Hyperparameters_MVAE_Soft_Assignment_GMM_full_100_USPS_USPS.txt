Dataset: USPS
--------------------------------------------
Model Name: MVAE_Soft_Assignment_GMM_full_100_USPS
==================================================================
MVAE_Soft_Assignment_GMM_full_100_USPS Part:
==================================================================
Random Seed = 100
--------------------------------------------
Device = cuda:0
--------------------------------------------
Number of Epochs = 2000
--------------------------------------------
Learning Rate = 0.0001
--------------------------------------------
Step Size = 20
--------------------------------------------
Input Dimension = 256
--------------------------------------------
Encoder Hidden Dimension 1 = 500
--------------------------------------------
Encoder Hidden Dimension 2 = 500
--------------------------------------------
Encoder Hidden Dimension 3 = 2000
--------------------------------------------
Latent Dimension = 5
--------------------------------------------
Decoder Hidden Layer 1 = 2000
--------------------------------------------
Decoder Hidden Layer 2 = 500
--------------------------------------------
Decoder Hidden Layer 3 = 500
--------------------------------------------
Output Dimension = 256
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 100
--------------------------------------------
Batch Size (Test) = 100
--------------------------------------------
gamma = 0.9
==================================================================

GMM Part:
==================================================================
Number of Components= 10
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-20
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 2001
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[-103.4259,  -93.4690,  -98.8859,  -99.5157,  -93.4878, -171.2560,
         -102.0905,  -96.1660,  -99.7315,  -99.5180],
        [-149.4576, -157.1404, -157.0957, -162.3749, -156.7140, -136.6801,
          -96.5525,  -95.3519,  -98.6333,  -97.7250],
        [-161.3870, -150.2701, -161.2451, -141.4985, -166.4287, -150.1316,
         -105.4776, -110.6478, -107.1820, -106.2550],
        [-109.7500, -123.0243, -106.4884, -123.9571, -127.2521, -101.5161,
          -82.8801,  -82.5583,  -84.3035,  -82.5896],
        [ -77.1925,  -78.9374,  -75.7813,  -83.0219,  -74.3220,  -83.0403,
          -89.5725,  -80.3456,  -73.0614,  -96.8363],
        [-113.7875, -117.6725, -132.3490, -120.8297, -123.2767, -108.8670,
         -164.3401, -175.4027, -178.7106, -167.7617],
        [-140.2999, -120.4041, -120.2146, -139.7135, -123.0684, -127.4478,
          -95.3798,  -97.8027,  -92.5867,  -93.5671],
        [-120.3182, -134.5300, -131.0927, -118.5999, -134.8387, -143.5547,
         -112.2834, -113.7152, -110.0483, -108.9426],
        [ -59.3484,  -61.6961,  -59.6290,  -90.9123,  -61.0563,  -66.5627,
          -82.4277,  -75.7918,  -60.6834,  -99.4579],
        [ -98.5319,  -94.6888, -101.8803,  -99.0272,  -98.4909, -111.4897,
         -118.6968,  -97.4108, -115.2123, -104.1057],
        [-146.5971, -132.1069, -150.8065, -148.6730, -141.6885, -128.2509,
         -102.3585, -102.3376, -101.9885, -107.4471],
        [-110.8558, -105.0632, -116.3950, -111.9534, -113.8978, -100.3564,
         -137.1569, -127.3189, -144.3479, -132.4467],
        [-147.0985, -144.7481, -152.8457, -142.7588, -162.8287, -133.5979,
         -100.8184, -103.4299, -102.4415, -101.5369],
        [ -72.5667,  -70.2218,  -71.5380,  -75.7751,  -70.0787,  -91.0511,
          -80.0075,  -69.7223,  -69.9707,  -82.3249],
        [-116.5882, -118.9246, -119.5311, -116.2751, -127.5167, -124.0127,
         -128.3122, -131.9528, -127.3903, -129.6379],
        [-130.7956, -105.8419, -134.6898, -122.4226, -129.0308,  -99.5154,
         -132.4268, -129.0922, -134.5936, -129.8009],
        [ -41.3030,  -39.5862,  -38.2314,  -40.4247,  -43.1568,  -58.5910,
          -43.0904,  -41.2536,  -39.7796,  -42.0315],
        [-160.5142, -152.4521, -129.0709, -127.0429, -105.9747, -101.5122,
          -80.7522,  -78.1690,  -78.0981,  -78.5938],
        [ -68.5773,  -72.0800,  -70.0446,  -68.6529,  -71.2073,  -83.1574,
          -79.9133,  -70.8098,  -71.3018,  -80.8155],
        [ -40.6221,  -43.2402,  -40.6530,  -40.6279,  -41.1965,  -46.9424,
          -50.7337,  -44.5254,  -41.0331,  -44.0304],
        [-131.0540, -128.0693, -115.4409, -130.6448, -141.9364, -117.5110,
          -84.5470,  -85.2170,  -84.2376,  -85.0456],
        [ -80.5763,  -78.1589,  -81.1581,  -79.9273,  -78.7345, -111.0831,
          -86.8676,  -78.3469,  -80.3461,  -94.2489],
        [-123.0803, -119.6412, -120.5180, -121.9944, -129.8999, -120.8861,
         -201.6024, -173.0192, -182.3934, -182.0721],
        [-117.6558, -112.8940, -118.3801, -130.0280, -111.7733, -130.6517,
         -184.5457, -165.1176, -132.6756, -211.4092],
        [ -55.9711,  -66.4591,  -56.5460,  -74.9663,  -56.4285,  -67.6953,
          -75.6653,  -74.1528,  -60.6805, -100.6806],
        [-105.1316, -100.4714, -101.8545, -107.7168, -106.2242, -100.9026,
         -153.7898, -161.3579, -111.7124, -155.7879],
        [ -64.3588,  -71.2873,  -64.5853,  -73.1047,  -68.7539,  -66.6437,
         -106.0975,  -92.6725, -137.4605, -114.6719],
        [ -50.8985,  -50.2474,  -48.1246,  -48.2395,  -49.0386,  -49.4479,
          -54.7059,  -50.0625,  -49.0590,  -65.4256],
        [-109.9894, -100.2669, -101.8411, -104.7129, -129.9991, -117.3761,
         -135.3861, -141.5985, -122.0699, -135.0347],
        [-126.9563, -111.9778, -110.4314, -157.3628, -101.7401, -100.2440,
          -76.9998,  -84.8671,  -79.0118,  -78.4793],
        [-113.7429, -106.0844, -120.6236, -105.5856, -127.8202, -130.7056,
         -156.3637, -138.3459, -118.2983, -185.2864],
        [-128.6156, -131.5109, -136.4283, -141.6368, -153.5815, -113.9676,
          -91.2627,  -89.6500,  -89.7599,  -87.2477],
        [-109.7515, -113.1188, -118.2800, -106.1807, -114.7479, -100.6208,
         -107.5514, -108.9494, -107.0428, -120.8026],
        [ -44.1138,  -44.5442,  -46.4063,  -44.9863,  -46.0442,  -45.6481,
          -75.4207,  -45.4637,  -46.9816,  -53.0198],
        [-132.1209, -111.8581, -118.7245, -141.0936, -111.7723, -105.0156,
          -78.1677,  -81.4405,  -84.3669,  -85.8313],
        [ -70.9002,  -73.5962,  -73.3847,  -73.5547,  -70.9794,  -74.9213,
          -76.2527,  -73.3037,  -74.9457,  -78.3836],
        [ -39.2063,  -41.9000,  -39.1107,  -38.8532,  -39.0031,  -42.3096,
          -42.3052,  -44.9497,  -39.8350,  -58.1743],
        [ -66.4126,  -69.5960,  -70.8975,  -66.2404,  -73.0097,  -67.1125,
         -103.2753,  -92.1864,  -90.2498, -105.8408],
        [ -95.1200,  -92.3720,  -92.4986,  -91.2273,  -90.1718,  -91.7213,
         -121.9803, -120.0037, -125.9586, -131.3887],
        [-119.8277, -117.8822, -120.9479, -125.3708, -130.9856, -120.8490,
         -153.2693, -154.8372, -126.9088, -170.3685],
        [-129.0415, -127.9785, -131.0226, -131.1309, -133.7409, -125.5416,
         -164.5937, -167.6846, -154.6781, -141.4086],
        [-137.9233, -133.3297, -144.0684, -139.5834, -138.5503, -130.0849,
         -108.3239, -107.3030, -105.9222, -107.8324],
        [ -83.4059,  -92.9185,  -89.0821,  -72.8916,  -70.4384, -105.8762,
          -77.2093,  -70.9559,  -70.2171, -102.0339],
        [-125.8382, -122.8777, -128.2001, -134.5151, -139.1856, -122.0832,
         -140.4025, -137.9079, -131.9798, -134.2155],
        [-136.1462, -122.4199, -123.6696, -141.4066, -137.4867, -120.2402,
          -88.3438,  -90.9379,  -90.1383,  -88.4000],
        [-158.8978, -144.2562, -143.0247, -182.4453, -141.4600, -133.9431,
          -99.1432, -102.8927,  -97.9656,  -99.2784],
        [ -87.9039,  -89.2093,  -80.4035,  -75.8223,  -75.0336, -117.3039,
          -72.2625,  -69.8603,  -70.2310,  -73.6183],
        [-108.1851, -103.1259, -103.1049, -115.4675,  -91.6149, -100.9334,
          -90.8695,  -91.2319,  -97.9515,  -93.7558],
        [ -77.6908,  -75.9690,  -74.9446,  -80.7941,  -81.8809,  -78.3962,
          -99.3415, -104.8725,  -98.3223,  -98.4762],
        [ -85.3977,  -83.7354,  -91.2657,  -86.1282,  -87.2903,  -84.9655,
         -131.2062, -124.9086, -101.5373, -132.4138],
        [ -37.4966,  -38.6149,  -37.4902,  -37.8378,  -38.7747,  -42.1262,
          -38.8600,  -44.2894,  -58.3050,  -46.5862],
        [ -83.1744,  -82.8728,  -82.6240,  -88.8674,  -86.0516,  -98.0553,
         -136.4945,  -90.6317,  -89.5819, -103.4635],
        [ -99.3950, -125.9357, -107.8996, -104.7977,  -94.9024, -108.8777,
          -87.0481,  -86.1645,  -85.0727,  -83.4980],
        [-130.3167, -118.3198, -115.9705, -122.6028, -125.2391, -121.2835,
         -136.3315, -131.4563, -128.5324, -136.1793],
        [-156.0585, -139.5081, -132.3335, -143.8090, -146.3432, -127.5492,
          -99.9076,  -96.2578,  -97.7224,  -98.4324],
        [-108.6521, -105.7515, -110.9264, -106.8599, -119.8689, -127.5036,
         -160.5414, -152.6672, -136.6019, -141.3442],
        [-120.2270, -117.6497, -145.1984, -127.1040, -160.1939, -131.9005,
         -104.2915, -105.7126, -105.1492, -104.5940],
        [-101.3530, -100.1196,  -99.9114,  -97.7643, -145.0190,  -97.5303,
         -113.9811, -116.3216, -136.9102, -115.6951],
        [ -81.4968,  -76.1579,  -77.4891,  -80.2602,  -78.7635,  -77.1570,
          -80.9826,  -79.1015,  -76.8776,  -89.3655],
        [ -39.1398,  -39.9639,  -38.6560,  -38.8105,  -40.0481,  -64.7808,
          -46.0431,  -46.9264,  -40.5303,  -45.8559],
        [-120.3655, -103.2447, -113.2693, -114.5331, -130.3378,  -91.8274,
         -131.0900, -133.7714, -154.7280, -143.1576],
        [-112.4389,  -98.9158, -104.6249, -113.3397, -106.8768,  -95.9964,
          -99.9118,  -99.6771, -106.2954,  -95.2251],
        [-101.2185,  -95.5982,  -98.4508,  -92.2857,  -95.9814, -112.7618,
          -97.9295,  -91.2263,  -91.8701,  -95.1022],
        [ -48.1517,  -48.9724,  -48.4511,  -48.9182,  -50.1557,  -65.3515,
          -69.4452,  -50.3447,  -49.8613,  -59.5431],
        [-155.6808, -138.0867, -146.1155, -146.1730, -152.6545, -141.4324,
          -97.8646,  -98.7490, -100.3575,  -96.9755],
        [ -76.6509,  -89.3887,  -81.9743,  -79.5658, -115.0378,  -78.1989,
          -92.1039,  -88.3056, -108.4247,  -95.1714],
        [ -99.6332,  -97.2922, -101.3594, -101.2744,  -91.7460, -100.6495,
          -91.0900,  -88.7010,  -89.9090,  -95.9621],
        [-104.4969, -110.6632, -115.7182, -113.6920, -135.9869, -102.9993,
         -162.1871, -153.3815, -173.4805, -157.9431],
        [-216.4047, -201.5233, -199.5315, -212.2694, -170.2847, -160.5298,
         -109.5211, -114.3510, -117.0588, -111.9766],
        [-105.9005, -103.1753, -102.1446, -106.5258, -105.5019, -104.7116,
         -150.3227, -136.4782, -152.8003, -129.8836],
        [ -91.4191,  -91.5779,  -92.9354,  -96.7684,  -94.2851,  -90.7295,
         -191.4545, -137.3251,  -98.0582, -189.6759],
        [-133.4580, -121.0807, -123.0389, -134.5998, -122.7667, -112.3416,
          -82.8185,  -83.7031,  -85.8312,  -81.8542],
        [-146.5496, -152.5583, -168.5949, -163.4164, -153.3636, -135.1838,
          -96.6056, -100.4312,  -95.0824,  -95.8257],
        [-171.6634, -143.3591, -141.2603, -156.7462, -106.9230, -125.7036,
          -84.9716,  -91.0945,  -88.7981,  -86.3399],
        [-123.0668, -119.0505, -117.9385, -125.6048, -116.6118, -119.6671,
         -194.9100, -143.7025, -118.8230, -218.7269],
        [ -52.0683,  -48.7787,  -50.6225,  -51.1144,  -50.1321,  -50.4751,
          -53.3494,  -54.9472,  -57.8145,  -63.7169],
        [ -49.5048,  -48.4789,  -49.2011,  -49.5829,  -53.6572,  -48.7678,
          -54.7591,  -54.7142,  -53.9824,  -68.9020],
        [ -80.0411,  -67.2611,  -78.7496,  -84.2316,  -67.3795,  -74.3467,
          -88.8805,  -88.4160,  -71.7368, -105.6473],
        [-172.2928, -159.5888, -167.5906, -193.2292, -193.7628, -140.0853,
         -110.3335, -116.3623, -110.7336, -106.4171],
        [-119.4736, -154.2871, -156.8677, -166.0299, -119.0724, -117.4684,
          -84.4748,  -82.9271,  -82.0625,  -82.1276],
        [-123.7926, -127.4410, -106.4580, -107.1237, -101.9282, -156.2810,
         -103.4848,  -99.1850,  -97.7402, -101.5088],
        [-107.6793, -100.3445,  -99.0652, -102.3703, -103.2650, -116.3649,
         -141.5817, -110.8490, -106.4384, -153.8072],
        [-103.5997, -103.6503, -102.4693,  -93.6898,  -91.1319, -160.1922,
          -96.5888,  -91.1969,  -83.7992,  -98.2295],
        [ -97.8797, -111.2856,  -91.4222,  -93.9208,  -85.3259, -123.7207,
          -94.9487,  -86.5211,  -87.0927,  -96.5361],
        [ -48.0100,  -49.7971,  -47.8604,  -48.0634,  -48.5080,  -50.6095,
          -65.0814,  -49.7512,  -52.7760,  -50.7961],
        [-127.7133, -128.6487, -123.9819, -146.0124,  -97.8256, -109.4012,
          -88.3546,  -88.6537,  -88.7997,  -87.6658],
        [-118.9155, -119.6889, -122.7793, -122.4440, -121.9476, -116.6768,
         -132.5473, -130.1329, -122.2912, -133.8517],
        [-114.9438, -111.5688, -132.5538, -103.5503, -100.3807, -168.1816,
          -95.2412, -101.3175, -100.2066,  -97.5500],
        [ -68.6286,  -71.5156,  -69.1113,  -69.2484,  -75.7938,  -79.8813,
          -91.9206,  -98.8724,  -74.1141,  -97.5652],
        [-136.7668, -119.8995, -117.4112, -119.7493, -116.9615, -121.2559,
         -172.3018, -170.6272, -155.8438, -166.8759],
        [-120.4772, -143.3765, -139.6554, -108.2438, -103.6205, -134.9388,
          -81.0258,  -78.8314,  -82.1955,  -84.5144]], device='cuda:0',
       grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([91, 10, 5]), scale: torch.Size([91, 10, 5]))
=============================================================================================
Decoder Loss (Test) = tensor([[-111.1945, -119.0234, -112.8653, -111.2797, -110.0689, -133.0682,
         -104.0092, -109.2220, -104.0452, -104.3418],
        [ -56.2911,  -57.1413,  -54.9546,  -61.1341,  -53.6687,  -60.9555,
          -60.5295,  -56.4902,  -53.8990,  -58.9557],
        [ -87.7497,  -80.2679,  -86.2594,  -84.1250,  -84.8685,  -79.0252,
         -123.2184, -108.0977, -131.3571, -113.4548],
        [ -69.0089,  -68.8436,  -69.2295,  -69.0217,  -69.2683,  -90.3975,
          -81.4526,  -72.0498,  -68.9959,  -89.2561],
        [ -96.7858, -102.9397,  -93.6370,  -92.9503,  -93.4095, -150.7609,
          -96.0296,  -95.6042,  -92.1236, -115.5906],
        [-125.0800, -148.5795, -133.0602, -112.6269, -106.9028, -167.0231,
          -95.0394,  -95.2758,  -95.3968,  -95.3743],
        [ -48.0668,  -48.0820,  -47.8631,  -48.7894,  -48.5283,  -48.9445,
          -52.7775,  -49.8435,  -48.1343,  -61.7755]], device='cuda:0')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([7, 10, 5]), scale: torch.Size([7, 10, 5]))
=============================================================================================
