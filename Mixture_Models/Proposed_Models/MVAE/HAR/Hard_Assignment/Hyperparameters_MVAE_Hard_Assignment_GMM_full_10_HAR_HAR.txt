Dataset: HAR
--------------------------------------------
Model Name: MVAE_Hard_Assignment_GMM_full_10_HAR
==================================================================
MVAE_Hard_Assignment_GMM_full_10_HAR Part:
==================================================================
Random Seed = 10
--------------------------------------------
Device = cuda:2
--------------------------------------------
Number of Epochs = 2000
--------------------------------------------
Learning Rate = 0.0001
--------------------------------------------
Step Size = 20
--------------------------------------------
Input Dimension = 561
--------------------------------------------
Encoder Hidden Dimension 1 = 500
--------------------------------------------
Encoder Hidden Dimension 2 = 500
--------------------------------------------
Encoder Hidden Dimension 3 = 2000
--------------------------------------------
Latent Dimension = 5
--------------------------------------------
Decoder Hidden Layer 1 = 2000
--------------------------------------------
Decoder Hidden Layer 2 = 500
--------------------------------------------
Decoder Hidden Layer 3 = 500
--------------------------------------------
Output Dimension = 561
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 100
--------------------------------------------
Batch Size (Test) = 100
--------------------------------------------
gamma = 0.9
==================================================================

GMM Part:
==================================================================
Number of Components= 6
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-20
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 2001
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[ -568.5227,  -528.6038,  -545.7772, -1979.3456,  -618.7589,  -519.5886],
        [ -590.8395,  -519.9839,  -614.7383,  -595.2372,  -614.7643,  -532.3420],
        [ -557.7655,  -526.1566,  -593.9384,  -535.5458,  -536.4662,  -533.6779],
        [ -553.2098,  -528.1356,  -669.1283, -1195.0414,  -585.0926,  -519.0873],
        [ -580.8463,  -525.2349,  -558.9760,  -533.3749,  -618.6284,  -531.8408],
        [ -533.7704,  -538.8066,  -528.5198,  -539.6819,  -525.0966,  -584.8738],
        [ -543.7329,  -529.8596,  -589.0699,  -578.8176,  -591.6878,  -520.7750],
        [ -542.1027,  -536.4218,  -584.9479,  -547.8777,  -548.8068,  -524.5299],
        [ -556.8450,  -527.9203,  -725.6971,  -712.2374,  -586.0818,  -520.0978],
        [ -524.5716,  -535.8305,  -522.9236,  -603.4372,  -523.6160,  -565.8619],
        [ -548.8140,  -531.5555,  -596.8739,  -523.1379,  -593.7727,  -520.3038],
        [ -557.1265,  -523.7775,  -696.2964,  -605.5060,  -598.6315,  -519.5960],
        [ -523.4763,  -526.5135,  -526.3860,  -524.6610,  -525.5322,  -548.2667],
        [ -526.4935,  -556.5636,  -591.1372,  -535.1954,  -521.1948,  -554.2963],
        [ -551.0153,  -528.3989,  -599.0791,  -521.0183,  -532.9459,  -521.2740],
        [ -535.4443,  -531.6814,  -524.3674,  -529.9246,  -531.3552,  -522.3198],
        [ -547.8300,  -530.0526,  -633.9251, -1032.9521,  -523.3126,  -520.9322],
        [ -520.7786,  -525.4607,  -523.2442,  -553.7087,  -523.3754,  -555.6224],
        [ -522.6839,  -553.2056,  -576.4742,  -581.9236,  -529.5496,  -565.3442],
        [ -539.8193,  -576.9069,  -699.4459,  -687.9861,  -531.7375,  -693.0434],
        [ -518.6879,  -546.6468,  -547.6124,  -547.4415,  -545.6678,  -601.5967],
        [ -545.3218,  -525.1418,  -640.7745, -1265.0305,  -609.8511,  -529.3652],
        [ -523.7048,  -534.4189,  -545.7417,  -554.0477,  -543.4960,  -527.8485],
        [ -524.3939,  -525.0789,  -532.2958,  -525.1834,  -528.2726,  -573.8583],
        [ -523.8827,  -538.1166,  -584.3176,  -539.4666,  -522.1783,  -589.8920],
        [ -525.6495,  -534.3975,  -527.1917,  -609.2697,  -530.5662,  -618.2475],
        [ -568.0750,  -530.5181,  -594.3848,  -551.4596,  -523.4348,  -521.4694],
        [ -531.4785,  -555.3922,  -642.0907,  -613.5935,  -525.1710,  -597.6562],
        [ -566.3927,  -528.1934,  -529.4918,  -559.2404,  -612.4326,  -518.2508],
        [ -522.5607,  -540.2587,  -711.0378,  -527.1420,  -526.4429,  -546.4667],
        [ -524.2852,  -539.8793,  -524.8958,  -568.0026,  -524.1304,  -560.6982],
        [ -524.7715,  -535.7889,  -527.8726,  -590.9292,  -527.8936,  -570.7931],
        [ -521.5575,  -538.1122,  -524.4826,  -522.8174,  -524.9889,  -544.8213],
        [ -522.3828,  -536.6707,  -559.1206,  -571.3749,  -526.5755,  -559.8300],
        [ -553.3886,  -531.1292,  -598.0674,  -529.3894,  -603.6954,  -522.7225],
        [ -524.6106,  -533.5796,  -575.5957,  -591.2661,  -531.0607,  -573.6840],
        [ -524.7603,  -536.8919,  -524.5349,  -538.7898,  -523.7842,  -559.5671],
        [ -524.3899,  -547.2335,  -578.4863,  -525.7513,  -560.2399,  -553.4416],
        [ -525.3630,  -528.0011,  -524.9169,  -586.8186,  -527.1995,  -560.2463],
        [ -531.4888,  -531.4584,  -564.1869,  -531.1083,  -560.4158,  -521.0443],
        [ -545.3340,  -530.8798,  -631.0917,  -697.1520,  -606.2993,  -518.7398],
        [ -521.2856,  -536.5804,  -543.5587,  -558.7059,  -547.0364,  -530.9553],
        [ -550.9648,  -532.5551,  -605.1876,  -821.6423,  -606.2440,  -522.2784],
        [ -559.3055,  -532.2883,  -628.5761,  -664.6031,  -531.3213,  -521.5858],
        [ -571.1350,  -531.1392,  -627.3533,  -626.5281,  -531.9125,  -520.7393],
        [ -521.6207,  -548.0841,  -528.3103,  -578.4070,  -531.1075,  -553.1696],
        [ -552.9255,  -526.0796,  -598.2170,  -547.7635,  -589.9843,  -537.3984],
        [ -541.3827,  -521.0917,  -576.0148,  -540.0132,  -620.0808,  -529.8163],
        [ -578.0846,  -532.8074,  -548.8842,  -531.3082,  -527.6079,  -522.8519],
        [ -522.3071,  -530.8906,  -604.1110,  -524.9095,  -524.1908,  -586.8538],
        [ -579.9282,  -519.3419,  -591.1059,  -526.3625,  -618.1793,  -529.2071],
        [ -586.8582,  -531.1052,  -580.2933, -1348.6740,  -603.8063,  -523.3320]],
       device='cuda:2', grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([52, 6, 5]), scale: torch.Size([52, 6, 5]))
=============================================================================================
Decoder Loss (Test) = tensor([[-526.3743, -527.5818, -526.5262, -525.0347, -526.4091, -572.0527],
        [-528.2449, -530.2205, -528.7570, -528.3265, -527.1942, -587.7383],
        [-527.2166, -530.3210, -527.2838, -528.3427, -526.2537, -583.1224],
        [-524.8291, -526.5325, -524.7065, -524.7777, -524.4695, -568.9779],
        [-522.8999, -526.2491, -523.8480, -523.4676, -523.2634, -564.0853],
        [-522.8546, -527.5153, -525.1115, -523.9860, -524.2584, -555.1578],
        [-524.4586, -526.9592, -524.4470, -524.4753, -523.9730, -563.2952],
        [-526.2409, -527.4832, -526.6437, -525.8114, -526.6796, -570.9284],
        [-521.6442, -531.7363, -530.7454, -524.5112, -528.3187, -541.9740],
        [-521.6395, -531.8638, -531.2144, -525.4358, -528.0079, -541.2749],
        [-521.3384, -533.1398, -534.5166, -526.0323, -530.5226, -536.9250],
        [-521.0955, -532.9105, -533.6455, -525.3901, -530.0519, -536.3091],
        [-520.0815, -533.5479, -531.4397, -524.8608, -527.8894, -538.6605],
        [-521.1218, -532.9492, -533.9185, -524.7689, -530.8457, -537.7251],
        [-520.4344, -531.5345, -532.8464, -524.3021, -529.5858, -537.0492],
        [-520.6121, -533.8019, -531.6506, -524.6304, -529.3063, -540.5958],
        [-522.0204, -532.3980, -531.6772, -525.7781, -529.3704, -540.4046],
        [-521.8502, -531.6847, -530.7250, -525.4792, -528.5002, -542.6501],
        [-521.7963, -531.9645, -532.2482, -525.9689, -529.4686, -541.2736],
        [-522.4283, -530.2321, -529.4070, -525.8760, -526.9476, -545.6132],
        [-521.7550, -530.6395, -529.6296, -524.9382, -527.2642, -544.5742],
        [-523.4951, -531.6086, -531.1736, -527.0718, -529.2440, -548.3155],
        [-522.9691, -530.1963, -529.4537, -526.4896, -527.7051, -550.2711],
        [-521.7000, -531.6615, -529.6089, -525.3661, -527.5535, -544.4659],
        [-521.8097, -531.5296, -529.8200, -525.4200, -527.6949, -545.4878],
        [-522.4642, -533.6942, -532.0504, -525.3247, -529.8973, -544.0854],
        [-521.4333, -533.5643, -531.7034, -524.5667, -529.4220, -541.8209],
        [-521.3037, -531.6866, -532.6906, -524.0389, -530.1616, -538.4872],
        [-524.4057, -525.6801, -524.6931, -523.6024, -524.1230, -564.3878],
        [-525.5224, -526.0619, -526.1422, -524.2617, -525.9117, -566.6440],
        [-523.9872, -524.5239, -524.5650, -523.1758, -524.1760, -566.9421],
        [-525.2891, -527.4824, -526.9902, -525.9902, -526.4980, -565.5825],
        [-523.7518, -526.3538, -525.5851, -524.6769, -525.1557, -566.4998],
        [-523.1997, -526.6114, -523.2083, -523.8620, -522.9282, -571.7861],
        [-522.0693, -525.8471, -522.6866, -523.1055, -522.4027, -565.0203],
        [-522.8046, -525.4634, -522.9174, -523.7061, -522.5161, -565.0099],
        [-525.4950, -526.3631, -525.4563, -524.5809, -525.0197, -565.1865],
        [-524.2325, -531.8199, -531.6646, -527.5010, -529.6193, -548.5638],
        [-522.3870, -529.7198, -529.2357, -525.2482, -527.1323, -546.3367],
        [-521.0947, -530.2399, -530.1923, -523.4075, -527.9133, -541.7576],
        [-520.4578, -531.6912, -530.7556, -524.6001, -528.2864, -540.1631],
        [-519.9125, -532.5115, -530.7591, -524.2728, -528.0338, -539.0095],
        [-521.7629, -533.2083, -530.5134, -525.2345, -528.4548, -542.9939],
        [-521.6436, -530.0363, -528.6412, -524.8541, -527.2406, -548.7032],
        [-522.1021, -530.0306, -529.8101, -524.5475, -528.4583, -545.4847],
        [-521.9163, -531.7715, -532.2346, -523.9402, -529.8764, -539.6292],
        [-521.3417, -529.8242, -530.8074, -522.9338, -528.7026, -540.9304]],
       device='cuda:2')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([47, 6, 5]), scale: torch.Size([47, 6, 5]))
=============================================================================================
