# -*- coding: utf-8 -*-
"""MVAE_Model_Thesis_Soft_Assignment_Fashion_MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FOqyyZNoNZiHZfNv7aFR1AQ1o6GN_Vml

# **Mixtures of VAE Model using Fashion MNIST Dataset**
"""

# This is the General Code

"""### **Libraries**"""

# Commented out IPython magic to ensure Python compatibility.
import warnings
warnings.filterwarnings("ignore")

# Debugger
import pdb

import time

# math and numpy
import itertools
import decimal
import mpmath
import math
import numpy as np
import random

# pandas
import pandas as pd
from pandas import DataFrame
from collections import Counter

# matplot
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from IPython import display
display.set_matplotlib_formats("svg")
# %matplotlib inline
import matplotlib.patches as mpatches

# scipy
import scipy.linalg as la
from scipy.stats import entropy
from scipy.stats import norm
from scipy.stats import multivariate_normal
import scipy.io as scio

# sklearn
from sklearn import datasets
from sklearn import metrics
from sklearn.mixture import GaussianMixture
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix
from sklearn.cluster import KMeans
from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score, accuracy_score
from scipy.optimize import linear_sum_assignment
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA


# torch
import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
import torchvision
from torch.optim.lr_scheduler import StepLR
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torch.utils.data import DataLoader, TensorDataset
from torchvision.utils import make_grid
from torch.distributions.multivariate_normal import MultivariateNormal
from torch.distributions.normal import Normal
from torch.distributions.mixture_same_family import MixtureSameFamily
from torch.distributions.categorical import Categorical
from torch.distributions.wishart import Wishart
from torch.distributions.dirichlet import Dirichlet
from torch.distributions import Bernoulli
from torch.utils.data import Dataset
import torchvision.models as models
from torchvision.models import resnet50
from torchvision.datasets import STL10

"""### **Device**"""

# start_time = time.time()
device=torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("Device:", device, "\n")
print("PyTorch Version:", torch.__version__, "\n")
print("Total Number of GPUs: ", torch.cuda.device_count())


"""## **Hyperparameters**"""

# VAE Part

pixel_size=[28, 28]
num_epochs=2000
learning_rate=1e-4
step_size=20
weight_decay=1e-5
input_dim=input_dim=pixel_size[0]*pixel_size[1]
encoder_hidden_dim_1=500
encoder_hidden_dim_2=500
encoder_hidden_dim_3=2000
latent_dim=7
decoder_hidden_dim_1=2000
decoder_hidden_dim_2=500
decoder_hidden_dim_3=500
output_dim=input_dim=pixel_size[0]*pixel_size[1]
train_batch_size=100
test_batch_size=100
gamma=0.9


# GMM Part
num_components=10
num_iterations=20
epsilon=1e-20
decimal.getcontext().prec = 28
clustering_method="GMM"
covariance_type="full"
GMM_interval=50


# Set seeds for NumPy, PyTorch, and Python's random module
random_seed=20
np.random.seed(random_seed)
torch.manual_seed(random_seed)
torch.cuda.manual_seed(random_seed)
random.seed(random_seed)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False


# Early Stopping
max_patience=num_epochs+1
best_test_loss=float("inf")
patience_counter=0

# File Name
Dataset_name = "Fashion_MNIST"
Model="MVAE"
Assignment="Soft_Assignment"

Model_name= f"{Model}_{clustering_method}_{covariance_type}_random_seed_{random_seed}_latent_dim_{latent_dim}_{Dataset_name}_dataset"

"""# **Fashion MNIST Dataset**

### **Summary of Dataset**
"""

data_dir = '~/Desktop/Fashion_MNIST_Dataset/data'
t = transforms.Compose([transforms.ToTensor()])
trainset = datasets.FashionMNIST(data_dir, train=True, download=True, transform=t)
testset  = datasets.FashionMNIST(data_dir, train=False, download=True, transform=t)

print("Training Set Size: ", len(trainset), "\n")
print("Test Set Size: ", len(testset), "\n")
print("Maximum Training Pixel: ", torch.max(trainset.data), "\n")
print("Minimum Training Pixel: ", torch.min(trainset.data), "\n")
print("Maximum Training Value: ", torch.max(trainset[0][0]), "\n")
print("Minimum Training Value: ", torch.min(trainset[0][0]), "\n")
print("5th Training Target: ", trainset[5][1], "\n")
print("First Training Example: \n\n", trainset.data[0].shape, "\n")
print("First Training Targets: \n\n", trainset.targets[0], "\n")

with open('Summary_Statistics.txt', 'w') as file:
    file.write(f"Training Set Size: {len(trainset)}\n")
    file.write(f"Test Set Size: {len(testset)}\n")
    file.write(f"Maximum Training Pixel: {torch.max(trainset.data)}\n")
    file.write(f"Minimum Training Pixel: {torch.min(trainset.data)}\n")
    file.write(f"Maximum Training Value: {torch.max(trainset[0][0])}\n")
    file.write(f"Minimum Training Value: {torch.min(trainset[0][0])}\n")
    file.write(f"5th Training Target: {trainset[5][1]}\n")
    file.write(f"First Training Example:\n\n{trainset.data[0].shape}\n")
    file.write(f"First Training Targets:\n\n{trainset.targets[0]}\n")

"""### **Data Loader**"""

train_loader=DataLoader(trainset, batch_size=train_batch_size, shuffle=True)
test_loader=DataLoader(testset, batch_size=test_batch_size, shuffle=False)
print("Number of Iterations in Each Epoch (Training):", len(train_loader))
print("Number of Iterations in Each Epoch (Test):", len(test_loader))

with open(f'Summary_Statistics_Training_{Dataset_name}.txt', 'w') as file:
    file.write(f"Training Set Size: {len(trainset)}\n")
    file.write(f"Maximum Training Pixel: {torch.max(trainset.data)}\n")
    file.write(f"Minimum Training Pixel: {torch.min(trainset.data)}\n")
    file.write(f"Maximum Training Value: {torch.max(trainset[0][0])}\n")
    file.write(f"Minimum Training Value: {torch.min(trainset[0][0])}\n")
    file.write(f"5th Training Target: {trainset[5][1]}\n")
    file.write(f"First Training Example: {trainset.data[0].shape}\n")
    file.write(f"First Training Targets: {trainset.targets[0]}\n")
    file.write(f"Number of Iterations in Each Epoch (Training): {len(train_loader)}\n")


with open(f'Summary_Statistics_Test_{Dataset_name}.txt', 'w') as file:
    file.write(f"Test Set Size: {len(testset)}\n")
    file.write(f"Maximum Test Pixel: {torch.max(testset.data)}\n")
    file.write(f"Minimum Test Pixel: {torch.min(testset.data)}\n")
    file.write(f"Maximum Test Value: {torch.max(testset[0][0])}\n")
    file.write(f"Minimum Test Value: {torch.min(testset[0][0])}\n")
    file.write(f"5th Test Target: {testset[5][1]}\n")
    file.write(f"First Test Example Shape: {testset.data[0].shape}\n")
    file.write(f"First Test Targets Shape: {testset.targets[0]}\n")
    file.write(f"Number of Iterations in Each Epoch (Test): {len(test_loader)}\n")

"""## **Visualizing Images**"""

with torch.no_grad():
  # retrieve the next batch of data from a DataLoader
  image,target=next(iter(train_loader))
  image=image.cpu()
  image=image.clamp(0,1)
  image=image[:50]
  image=make_grid(image, 10, 5)
  image=image.numpy()
  image=np.transpose(image,(1,2,0))
  plt.imshow(image)
  plt.savefig(f'Random_Image_{Dataset_name}.png')
  plt.close()

"""# **Functions**

### **Clustering Accuracy with Linear Assignment**
"""

def cluster_acc_with_assignment(y_true, y_pred):
    y_true = y_true.astype(np.int64)
    assert y_pred.size == y_true.size
    D = max(y_pred.max(), y_true.max()) + 1
    w = np.zeros((D, D), dtype=np.int64)
    for i in range(y_pred.size):
        w[y_pred[i], y_true[i]] += 1
    row_ind, col_ind = linear_sum_assignment(w.max() - w)

    new_predicted_labels = np.array([col_ind[i] for i in y_pred])
    accuracy = sum([w[row, col] for row, col in zip(row_ind, col_ind)]) * 1.0 / y_pred.size
    return accuracy, new_predicted_labels

"""
### 
# **Mahalanobis Distance**
"""
def MD(x, mu, cov):
  x_minus_mu=x-mu
  inv_cov=torch.inverse(cov)
  left_term=torch.mm(x_minus_mu, inv_cov)
  distance=torch.mm(left_term, x_minus_mu.T)
  distance_sq=distance**2
  return distance_sq



"""# **MVAE Model**

### **Encoder Class**
"""

class Encoder(nn.Module):
  def __init__(self):
    super(Encoder, self).__init__()
    self.encoder = nn.Sequential(
        nn.Linear(input_dim, encoder_hidden_dim_1),
        nn.ReLU(),
        nn.Linear(encoder_hidden_dim_1, encoder_hidden_dim_2),
        nn.ReLU(),
        nn.Linear(encoder_hidden_dim_2, encoder_hidden_dim_3),
        nn.ReLU()
        )
    self.mu_layer = nn.Linear(encoder_hidden_dim_3, latent_dim * num_components)
    self.log_var_layer = nn.Linear(encoder_hidden_dim_3, latent_dim * num_components)

  def forward(self, x):
      x = self.encoder(x)
      mu_layer = self.mu_layer(x)
      log_var_layer = self.log_var_layer(x)
      mu_k = mu_layer.view(-1, num_components, latent_dim)
      log_var_k = log_var_layer.view(-1, num_components, latent_dim)
      return mu_k, log_var_k

"""### **Decoder Class**"""

class Decoder(nn.Module):
  def __init__(self):
    super(Decoder, self).__init__()
    self.decoder = nn.Sequential(
        nn.Linear(latent_dim, decoder_hidden_dim_1),
        nn.ReLU(),
        nn.Linear(decoder_hidden_dim_1, decoder_hidden_dim_2),
        nn.ReLU(),
        nn.Linear(decoder_hidden_dim_2, decoder_hidden_dim_3),
        nn.ReLU(),
        nn.Linear(decoder_hidden_dim_3, output_dim),
        nn.Sigmoid()
        )
  def forward(self, z):
      return self.decoder(z)

"""### **GMM Prior Class**"""

class GMM(nn.Module):
  def __init__(self, M, K):
    super(GMM, self).__init__()
    self.M=M
    self.K=K
    self.means=nn.Parameter(torch.randn(self.K, self.M))                         # [K, M]
    self.logvars=nn.Parameter(torch.randn(self.K, self.M))                       # [K, M]
    # self.weights=nn.Parameter(torch.ones(self.K)/self.K)                       # [K]
    self.weights=nn.Parameter(Dirichlet(torch.ones(self.K)).sample())            # [K]

  def get_params(self):
    return self.means, self.logvars, self.weights

  def log_prob(self, z):
    num_samples, num_components, num_features = z.shape                          # [B, K, M]
    mean, logvar, weight = self.get_params()
    weight=F.softmax(weight, dim=0)                                              # [K]
    mean=mean.reshape(1, num_components, latent_dim)                             # [1, K, M]
    logvar=logvar.reshape(1, num_components, latent_dim)                         # [1, K, M]
    prior_dist=Normal(loc=mean, scale=torch.exp(0.5*logvar))
    log_pi=torch.log(weight)
    prior_log_prob=prior_dist.log_prob(z).sum(dim=2)                             # [B, K]
    return log_pi, prior_log_prob                                                # [K], [B, K]

"""### **MVAE Class**"""

class MVAE(nn.Module):
  def __init__(self):
    super(MVAE, self).__init__()
    self.encoder=Encoder()
    self.decoder=Decoder()
    self.prior=GMM(M=latent_dim, K=num_components)

  def reparameterize(self, mu, log_var):
    if self.training:
      sigma=torch.exp(0.5*log_var)
      eps=torch.randn_like(sigma)
      z=mu+eps*sigma
      return z
    else:
      return mu

  def forward(self, x):
    x=x.view(-1, input_dim)
    encoder_mu_k, encoder_logvar_k=self.encoder(x)
    latent_z_k=self.reparameterize(encoder_mu_k, encoder_logvar_k)
    log_pi_k, log_normal_prior_k=self.prior.log_prob(latent_z_k)
    decoder_mu=self.decoder(latent_z_k)
    return decoder_mu, log_pi_k, log_normal_prior_k, latent_z_k, encoder_mu_k, encoder_logvar_k

"""### **MGVAE Model and Optimizer**"""

model=MVAE().to(device)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)
print(model)
num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print('\n \n Number of Estimated Parameters: %d' % num_params)

model_info=str(model)
file_path=f"Model_Architecture_{Model_name}_{Dataset_name}.txt"
with open(file_path, "w") as file:
    file.write("Model Architecture:\n")
    file.write(model_info)
    file.write("\n\nNumber of Estimated Parameters: %d\n" % num_params)

"""### **Shape of Encoder, Decoder and Prior**"""

"""# **Model Training**

### **Backpropagation and Parameter Updates**
"""

Training_Losses=[]
Test_Loss=[]
True_Labels=[]
Latent_embeddings_best=[]

# GMM Part
gmm_mean=[]
gmm_covariance=[]
simulated_gmm_mean = []
simulated_gmm_covariance = []
Accuracy_GMM=[]
Mean_Accuracy_GMM=[]
STD_Accuracy_GMM=[]
NMI_GMM=[]
ARI_GMM=[]
Posterior_EM=[]
Updated_GMM_Labels=[]

# Posterior Probability
Accuracy_Posterior=[]
NMI_Posterior=[]
ARI_Posterior=[]
Updated_Posterior_Labels=[]
Posterior_NN=[]

print(f"Training and Testing {Model_name} Model: {Dataset_name} Dataset: \n\n")

for epoch in range(num_epochs):

  ## Training Step
  model.train()
  Training_Losses.append(0)
  num_batches=0

  for training_features, _ in train_loader:
    optimizer.zero_grad()
    training_features=training_features.reshape(-1, input_dim)
    training_features=training_features.to(device)

    decoder_mu, log_pi_prior_k, log_normal_prior_k, latent_z_k, latent_mu_k, latent_logvar_k=model(training_features)
    decoder_mu=decoder_mu.to(device)                                             # [B, K, D]
    batch_size, batch_dim = training_features.shape

    # Encoder Likelihood: [B, K, M]

    q_z_y=Normal(loc=latent_mu_k, scale=torch.exp(0.5*latent_logvar_k))
    log_q_z_y=q_z_y.log_prob(latent_z_k).sum(dim=2)

    # Decoder Likelihood: [B, K, D]

    BCE_losses=[]
    for k in range(decoder_mu.size(1)):
      inputs_k=decoder_mu[:, k, :]
      targets_k=training_features
      BCE_k=-F.binary_cross_entropy(inputs_k, targets_k, reduction="none")
      BCE_losses.append(BCE_k)
    BCE_Loss=torch.stack(BCE_losses, dim=1)
    log_p_x_z = torch.sum(BCE_Loss, dim=2)

    ############################# Real-Valued Data #############################

    # log_p_x_z = -F.mse_loss(input=decoder_mu, target=training_features.reshape(batch_size, 1, input_dim), reduction="none")
    # log_p_x_z = torch.sum(log_p_x_z, dim=2)

    # p_x_z=Normal(loc=decoder_mu, scale=torch.ones_like(decoder_mu))
    # input=training_features.reshape(batch_size, 1, input_dim)
    # log_p_x_z=p_x_z.log_prob(input)
    # log_p_x_z=log_p_x_z.sum(dim=2)

    ############################################################################

    log_likelihood=(log_p_x_z+log_normal_prior_k+log_pi_prior_k-log_q_z_y)       # [B, K]


    ############### Closed Form: Posterior Probabilities: q(y_ik)###############

    # q_numerator=torch.exp(log_likelihood)                                      # [B, K]
    # q_denominator=torch.sum(q_numerator, dim=1)
    # q_denominator_reshape=q_denominator.reshape(batch_size, 1)
    # q_y_i_k=q_numerator/q_denominator_reshape                                  # [B, K]
    # index=q_y_i_k==0
    # q_y_i_k[index]+=epsilon

    q_y_i_k= []
    for row in log_likelihood:
      q_numerator= [decimal.Decimal(value.item()).exp() for value in row]
      q_denominator = sum(q_numerator)
      normalized_probability = [value / q_denominator for value in q_numerator]
      q_y_i_k.append(normalized_probability)
    q_y_i_k = torch.tensor(q_y_i_k, dtype=torch.float32).to(device)

    index=q_y_i_k==0
    q_y_i_k[index]+=epsilon

    log_lik=(log_likelihood-torch.log(q_y_i_k))                                  # [B, K]
    ELBO=(q_y_i_k*log_lik).sum(dim=(0,1))


    # Total Loss
    Total_Loss = -ELBO

    # Backpropagation
    Total_Loss.backward()
    optimizer.step()
    num_batches+=1
    Training_Losses[-1]+=Total_Loss.item()

  # Update the Learning Rate
  scheduler.step()

  # Average Losses
  Training_Losses[-1]/= num_batches

  ## Evaluation Step
  model.eval()
  Final_Test_Loss=0.0
  number_of_batches=0
  ground_truth_labels=[]
  latent_best_z=[]

  # Posterior Part
  posterior_probabilities=[]

  # GMM Part
  simulated_gmm_accuracies = []
  simulated_gmm_nmi = []
  simulated_gmm_ari = []
  simulated_predicted_labels_gmm = []
  simulated_posterior_probabilities_gmm = []
  reconstructions = []
  original_image = []
  Best_class=[]

  with torch.no_grad():
    for test_features, test_labels in test_loader:

      test_features=test_features.reshape(-1, input_dim)
      test_features=test_features.to(device)
      test_labels=test_labels.to(device)

      decoder_mu_test, log_pi_prior_k_test, log_normal_prior_k_test, latent_z_k_test, latent_mu_k_test, latent_logvar_k_test = model(test_features)
      decoder_mu_test=decoder_mu_test.to(device)
      ground_truth_labels.append(test_labels.cpu().numpy())
      batch_size, batch_dim = test_features.shape

      # Encoder Likelihood: Shape [B, K]

      q_z_y_test=Normal(loc=latent_mu_k_test, scale=torch.exp(0.5*latent_logvar_k_test))
      log_q_z_y_test = q_z_y_test.log_prob(latent_z_k_test).sum(dim=2)


      # Decoder Likelihood: Shape [B, K, D]

      BCE_losses_test=[]
      for k in range(decoder_mu_test.size(1)):
        inputs_k=decoder_mu_test[:, k, :]
        targets_k=test_features
        BCE_k=-F.binary_cross_entropy(inputs_k, targets_k, reduction="none")
        BCE_losses_test.append(BCE_k)
      BCE_Loss_test=torch.stack(BCE_losses_test, dim=1)
      log_p_x_z_test = torch.sum(BCE_Loss_test, dim=2)

      ############################# Real-Valued Data ###########################


      # log_p_x_z_test = -F.mse_loss(input=decoder_mu_test, target=test_features.reshape(batch_size, 1, input_dim), reduction="none")
      # log_p_x_z_test = torch.sum(log_p_x_z_test, dim=2)

      # p_x_z_test=Normal(loc=decoder_mu_test, scale=torch.ones_like(decoder_mu_test))
      # input_test=test_features.reshape(batch_size, 1, input_dim)
      # log_p_x_z_test=p_x_z_test.log_prob(input_test)
      # log_p_x_z_test=log_p_x_z_test.sum(dim=2)

      ##########################################################################

      likelihood_test=(log_p_x_z_test + log_normal_prior_k_test + log_pi_prior_k_test - log_q_z_y_test)


      ############### Closed Form: Posterior Probabilities: q(y_ik)#############

      # q_numerator_test = torch.exp(likelihood_test)                            # [B, K]
      # q_denominator_test=torch.sum(q_numerator_test, dim=1)                    # [B]
      # q_denominator_test=q_denominator_test.reshape(batch_size, 1)             # [B, 1]
      # q_y_i_k_test=q_numerator_test/q_denominator_test                         # [B, K]
      # index_test=q_y_i_k_test==0
      # q_y_i_k_test[index_test]+=epsilon

      q_y_i_k_test= []
      for row in likelihood_test:
        q_numerator= [decimal.Decimal(value.item()).exp() for value in row]
        q_denominator = sum(q_numerator)
        normalized_probability = [value / q_denominator for value in q_numerator]
        q_y_i_k_test.append(normalized_probability)
      q_y_i_k_test = torch.tensor(q_y_i_k_test, dtype=torch.float32).to(device)

      index_test=q_y_i_k_test==0
      q_y_i_k_test[index_test]+=epsilon

      ##########################################################################

      posterior_probabilities.append(q_y_i_k_test)

      ######################### Test ELBO  #########################

      # Test ELBO (Soft Assignment)

      log_lik_test=(likelihood_test-torch.log(q_y_i_k_test))
      ELBO_test=(q_y_i_k_test*log_lik_test).sum(dim=(0,1))
      best_k=torch.argmax(q_y_i_k_test, dim=1)
      latent_z=latent_z_k_test[torch.arange(batch_size), best_k]
      latent_best_z.append(latent_z)


      best_reconstruction = decoder_mu_test[torch.arange(batch_size), best_k]
      reconstructions.append(best_reconstruction)
      original_image.append(test_features)

      # Total Loss

      Total_Test_Loss = -ELBO_test
      Final_Test_Loss+= Total_Test_Loss.item()
      number_of_batches+= 1


    # Average Loss
    Final_Test_Loss/=number_of_batches
    Test_Loss.append(Final_Test_Loss)

    # Latent Space
    latent_best=torch.cat(latent_best_z, dim=0).cpu().detach().numpy()
    Latent_embeddings_best.append(latent_best)

    # True Labels
    ground_truth_labels_numpy = np.concatenate(ground_truth_labels)
    True_Labels.append(ground_truth_labels_numpy)

    ################## Clustering using Posterior Probability###################


    # Clustering using Posterior Probabilities (Soft Assignment)

    posterior_probabilities_q_y_i_k=torch.cat(posterior_probabilities, dim=0)
    predicted_labels_posterior=torch.argmax(posterior_probabilities_q_y_i_k, dim=1).cpu().numpy()
    Posterior_NN.append(posterior_probabilities_q_y_i_k)
    nmi_posterior = normalized_mutual_info_score(ground_truth_labels_numpy, predicted_labels_posterior)
    ari_posterior= adjusted_rand_score(ground_truth_labels_numpy, predicted_labels_posterior)
    NMI_Posterior.append(nmi_posterior)
    ARI_Posterior.append(ari_posterior)
    new_accuracy_posterior, new_predicted_labels_posterior = cluster_acc_with_assignment(ground_truth_labels_numpy, predicted_labels_posterior)
    Accuracy_Posterior.append(new_accuracy_posterior)
    Updated_Posterior_Labels.append(new_predicted_labels_posterior)


    ##################Clustering using Gaussian Mixture Model###################
    if (epoch + 1) % GMM_interval == 0:
        for i in range(num_iterations):
            # seed = np.random.randint(0, 100)
            gmm = GaussianMixture(n_components=num_components, covariance_type='full', random_state=i)
            predicted_labels_gmm = gmm.fit_predict(latent_best)
            posterior_probabilities_gmm = gmm.predict_proba(latent_best)
            simulated_posterior_probabilities_gmm.append(posterior_probabilities_gmm)
            nmi_gmm = normalized_mutual_info_score(ground_truth_labels_numpy, predicted_labels_gmm)
            ari_gmm = adjusted_rand_score(ground_truth_labels_numpy, predicted_labels_gmm)
            new_accuracy_gmm, new_predicted_labels_gmm = cluster_acc_with_assignment(ground_truth_labels_numpy,
                                                                                     predicted_labels_gmm)
            simulated_gmm_accuracies.append(new_accuracy_gmm)
            simulated_gmm_nmi.append(nmi_gmm)
            simulated_gmm_ari.append(ari_gmm)
            simulated_predicted_labels_gmm.append(new_predicted_labels_gmm)
            simulated_gmm_mean.append(gmm.means_)
            simulated_gmm_covariance.append(gmm.covariances_)

        average_gmm_accuracy = np.mean(simulated_gmm_accuracies)
        max_gmm_accuracy = np.max(simulated_gmm_accuracies)
        sd_gmm_accuracy = np.std(simulated_gmm_accuracies)
        Accuracy_GMM.append(max_gmm_accuracy)
        Mean_Accuracy_GMM.append(average_gmm_accuracy)
        STD_Accuracy_GMM.append(sd_gmm_accuracy)
        best_accuracy_index = simulated_gmm_accuracies.index(np.max(simulated_gmm_accuracies))
        Updated_GMM_Labels.append(simulated_predicted_labels_gmm[best_accuracy_index])
        Posterior_EM.append(simulated_posterior_probabilities_gmm[best_accuracy_index])
        NMI_GMM.append(simulated_gmm_nmi[best_accuracy_index])
        ARI_GMM.append(simulated_gmm_ari[best_accuracy_index])
        gmm_mean_list = simulated_gmm_mean[best_accuracy_index]
        gmm_covariance_list = simulated_gmm_covariance[best_accuracy_index]
        gmm_mean.append(gmm_mean_list)
        gmm_covariance.append(gmm_covariance_list)
    ####################################################################################################################

    # plot_confusion_matrix_gmm(ground_truth_labels_numpy, new_predicted_labels_gmm, epoch)
    # plot_confusion_matrix_posterior(ground_truth_labels_numpy, new_predicted_labels_posterior, epoch)

    # Reconstructed Digits

    if (epoch + 1) == num_epochs:
        reconstructions = torch.cat(reconstructions, dim=0)
        True_labels = np.array(True_Labels[-1])
        Predicted_labels = np.array(Updated_GMM_Labels[-1])
        Selected_Image = []

        for digits in range(num_components):
            Matching_index = np.where((True_labels == digits) & (Predicted_labels == digits))[0]
            Selected_Reconstruction = reconstructions[Matching_index]
            Selected_Reconstruction = Selected_Reconstruction[:10]
            Selected_Image.append(Selected_Reconstruction)

        Selected_Image = torch.cat(Selected_Image, dim=0)
        Reconstructed_Digits = Selected_Image.view(Selected_Image.size(0), 1, pixel_size[0], pixel_size[1])
        Reconstructed_Digits = Reconstructed_Digits.cpu()
        Reconstructed_Digits = Reconstructed_Digits.clamp(0, 1)
        Reconstructed_Digits = Reconstructed_Digits[: 100]
        Reconstructed_Digits = make_grid(Reconstructed_Digits, 10, 10)
        Reconstructed_Digits = Reconstructed_Digits.numpy()
        Reconstructed_Digits = np.transpose(Reconstructed_Digits, (1, 2, 0))
        plt.imshow(Reconstructed_Digits)
        plt.axis('off')
        plt.savefig(f'Reconstructed_Digits_{Model_name}_{Dataset_name}.png')
        plt.close()

        # Mismatch Digits

        Selected_Mismatch_Image = []

        for digits in range(num_components):
            Mismatching_index = np.where((True_labels == digits) & (Predicted_labels != digits))[0]
            Selected_Reconstruction = reconstructions[Mismatching_index]
            Selected_Reconstruction = Selected_Reconstruction[:10]
            Selected_Mismatch_Image.append(Selected_Reconstruction)

        Selected_Image = torch.cat(Selected_Mismatch_Image, dim=0)
        Reconstructed_Digits = Selected_Image.view(Selected_Image.size(0), 1, pixel_size[0], pixel_size[1])
        Reconstructed_Digits = Reconstructed_Digits.cpu()
        Reconstructed_Digits = Reconstructed_Digits.clamp(0, 1)
        Reconstructed_Digits = Reconstructed_Digits[: 100]
        Reconstructed_Digits = make_grid(Reconstructed_Digits, 10, 10)
        Reconstructed_Digits = Reconstructed_Digits.numpy()
        Reconstructed_Digits = np.transpose(Reconstructed_Digits, (1, 2, 0))
        plt.imshow(Reconstructed_Digits)
        plt.axis('off')
        plt.savefig(f'Reconstructed_Mismatch_Digits_{Model_name}_{Dataset_name}.png')
        plt.close()

        # Original Images (Random)

        original_test_image = torch.cat(original_image, dim=0)
        test_images = original_test_image.view(original_test_image.size(0), 1, pixel_size[0], pixel_size[1])
        test_images = test_images.cpu()
        test_images = test_images.clamp(0, 1)
        test_images = test_images[:50]
        test_images = make_grid(test_images, 10, 5)
        test_images = test_images.numpy()
        test_images = np.transpose(test_images, (1, 2, 0))
        plt.imshow(test_images)
        plt.axis('off')
        plt.savefig(f'Original_Random_Images_{Model_name}_{Dataset_name}.png')
        plt.close()

        # Reconstructed Images (Random)

        Reconstructions = reconstructions.view(reconstructions.size(0), 1, pixel_size[0], pixel_size[1])
        Reconstructions = Reconstructions.cpu()
        Reconstructions = Reconstructions.clamp(0, 1)
        Reconstructions = Reconstructions[: 50]
        Reconstructions = make_grid(Reconstructions, 10, 5)
        Reconstructions = Reconstructions.numpy()
        Reconstructions = np.transpose(Reconstructions, (1, 2, 0))
        plt.imshow(Reconstructions)
        plt.axis('off')
        plt.savefig(f'Reconstructed_Random_Images_{Model_name}_{Dataset_name}.png')
        plt.close()

    if (epoch+1)%GMM_interval==0:
        print(f"\n Step: [{epoch+1}/{num_epochs}] \t  Training Loss: {Training_Losses[-1]: 0.2f} \t Test Loss: {Final_Test_Loss: 0.2f} \
        \t Accuracy (GMM): {max_gmm_accuracy: 0.2f} \t Accuracy (Posterior): {new_accuracy_posterior: 0.2f} \t NMI (GMM): {simulated_gmm_nmi[best_accuracy_index]: 0.2f} \t NMI (Posterior): {nmi_posterior: 0.2f} \
        \t LR: {scheduler.get_last_lr()[0]:0.6f}")

"""### **Store All the Results**"""

hyperparameter_file_path = f'Hyperparameters_{Model_name}_{Dataset_name}.txt'
with open(hyperparameter_file_path, 'w') as file:
  file.write(f"Dataset: {Dataset_name}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Model Name: {Model_name}\n")
  file.write(f"==================================================================\n")
  file.write(f"{Model_name} Part:\n")
  file.write(f"==================================================================\n")
  file.write(f"Random Seed = {random_seed}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Device = {device}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Number of Epochs = {num_epochs}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Learning Rate = {learning_rate}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Step Size = {step_size}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Input Dimension = {input_dim}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Encoder Hidden Dimension 1 = {encoder_hidden_dim_1}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Encoder Hidden Dimension 2 = {encoder_hidden_dim_2}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Encoder Hidden Dimension 3 = {encoder_hidden_dim_3}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Latent Dimension = {latent_dim}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Decoder Hidden Layer 1 = {decoder_hidden_dim_1}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Decoder Hidden Layer 2 = {decoder_hidden_dim_2}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Decoder Hidden Layer 3 = {decoder_hidden_dim_3}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Output Dimension = {output_dim}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Weight Decay = {weight_decay}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Batch Size (Training) = {train_batch_size}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Batch Size (Test) = {test_batch_size}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"gamma = {gamma}\n")
  file.write(f"==================================================================\n")


  file.write("\nGMM Part:\n")
  file.write(f"==================================================================\n")
  file.write(f"Number of Components= {num_components}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Covariance Matrix= {covariance_type}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Number of Iterations= {num_iterations}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Epsilon = {epsilon}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Precision = {decimal.getcontext().prec }\n")
  file.write(f"==================================================================\n")

  file.write("\nEarly Stopping:\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Maximum Patience = {max_patience}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Best Test Loss = {best_test_loss}\n")
  file.write(f"--------------------------------------------\n")
  file.write(f"Patience Counters = {patience_counter}\n")
  file.write(f"==================================================================\n")

  file.write("\nLoss Functions:\n")
  file.write(f"==================================================================\n")
  file.write(f"Decoder Loss (Training) = {log_p_x_z}\n")
  file.write(f"=============================================================================================\n")
  file.write(f"Encoder Loss (Training) = {q_z_y}\n")
  file.write(f"=============================================================================================\n")
  file.write(f"Decoder Loss (Test) = {log_p_x_z_test}\n")
  file.write(f"=============================================================================================\n")
  file.write(f"Encoder Loss (Test) = {q_z_y_test}\n")
  file.write(f"=============================================================================================\n")

"""### **Save Results**"""

with open(f'Training_Losses_{Model_name}_{Dataset_name}.txt', 'w') as file:
    for loss in Training_Losses:
        file.write(str(loss) + '\n')

with open(f'Test_Losses_{Model_name}_{Dataset_name}.txt', 'w') as file:
  for loss1 in Test_Loss:
    file.write(str(loss1) + '\n')

with open(f'Posterior_Accuracy_New_{Model_name}_{Dataset_name}.txt', 'w') as file:
  for Posterior_Accuracy in Accuracy_Posterior:
    file.write(str(Posterior_Accuracy) + '\n')

with open(f'NMI_Posterior_{Model_name}_{Dataset_name}.txt', 'w') as file:
  for NMI_Posteriors in NMI_Posterior:
    file.write(str(NMI_Posteriors) + '\n')

with open(f'ARI_Posterior_{Model_name}_{Dataset_name}.txt', 'w') as file:
  for ARI_Posteriors in ARI_Posterior:
    file.write(str(ARI_Posteriors) + '\n')

with open(f'Accuracy_GMM_{Model_name}_{Dataset_name}.txt', 'w') as file:
  for loss4 in Accuracy_GMM:
    file.write(str(loss4) + '\n')

with open(f'NMI_GMM_{Model_name}_{Dataset_name}.txt', 'w') as file:
  for loss5 in NMI_GMM:
    file.write(str(loss5) + '\n')

with open(f'ARI_GMM_{Model_name}_{Dataset_name}.txt', 'w') as file:
  for loss6 in ARI_GMM:
    file.write(str(loss6) + '\n')

"""### **Model Parameters**"""

# print(f"Model Parameters: \n\n", model.parameters)
param_list = [*model.parameters()]
# print(f"\n Parameter List: \n\n", param_list)
weights=param_list[-1]
print(f"Mixture Weights without Softmax: \n\n", weights)
print(f"\n Mixture Weights: \n\n", F.softmax(weights))

results_file_path = f'GMM_Parameters_{Model_name}_{Dataset_name}.txt'


with open(results_file_path, 'w') as file:
    weights = param_list[-1]
    file.write("\nMixture Weights without Softmax:\n")
    file.write(str(weights) + '\n')

    file.write("\nMixture Weights:\n")
    softmax_weights = F.softmax(weights)
    file.write(str(softmax_weights) + '\n')

    logvariances = param_list[-2]
    file.write("\nLog Variances:\n")
    file.write(str(logvariances) + '\n')

    file.write("\nVariances:\n")
    variances = torch.exp(logvariances)
    file.write(str(variances) + '\n')

    means = param_list[-3]
    file.write("\nGMM Means:\n")
    file.write(str(means) + '\n')

    true_labels_frequency = Counter(True_Labels[-1])
    file.write("\nTrue Labels Frequency:\n")
    file.write(str(true_labels_frequency) + '\n')

    GMM_labels_frequency = Counter(Updated_GMM_Labels[-1])
    file.write("\nPredicted Labels Frequency:\n")
    file.write(str(GMM_labels_frequency) + '\n')

    posterior_labels_frequency = Counter(Updated_Posterior_Labels[-1])
    file.write("\nPosterior Labels Frequency:\n")
    file.write(str(posterior_labels_frequency) + '\n')

results_file_path_2 = f'Model_Parameters_{Model_name}_{Dataset_name}.txt'
with open(results_file_path_2, 'w') as file:
    Parameters = param_list
    file.write("\nParameter List:\n")
    file.write(str(Parameters) + '\n')

"""# **Results**

### **Clustering Accuracy**
"""

print("\n Loss:")
print("-"*50)
print("\n Final Training Loss:", Training_Losses[-1])
print("\n Final Test Loss:", Test_Loss[-1])
print("-"*50)
print("\n Posterior Probability Results:")
print(f"\n Accuracy (Posterior): {Accuracy_Posterior[-1]: 0.3f}")
print(f'\n NMI (Posterior): {NMI_Posterior[-1]: 0.2f}')
print(f'\n ARI (Posterior): {ARI_Posterior[-1]: 0.2f}')
print("-"*50)

print("-"*50)
print("\n GMM Results:")
print(f"\n Clustering accuracy (GMM): {Accuracy_GMM[-1]: 0.3f}")
print(f'\n NMI (GMM): {NMI_GMM[-1]: 0.2f}')
print(f'\n ARI (GMM): {ARI_GMM[-1]: 0.2f}')
print("-"*50)

overall_avg_accuracy_posterior = np.mean(Accuracy_Posterior)
overall_std_accuracy_posterior = np.std(Accuracy_Posterior)

print("Overall Mean Accuracy (Posterior Probability):", overall_avg_accuracy_posterior, "\n")
print("Overall STD Accuracy (Posterior Probability):", overall_std_accuracy_posterior, "\n")


overall_avg_accuracy_gmm = np.mean(Accuracy_GMM)
overall_std_accuracy_gmm = np.std(Accuracy_GMM)
print("Overall Mean Accuracy (GMM)", overall_avg_accuracy_gmm, "\n")
print("Overall STD Accuracy (GMM)", overall_std_accuracy_gmm, "\n")

text_to_save =f"\n Final Training Loss: {Training_Losses[-1]}\n" \
              f"\n Final Test Loss: {Test_Loss[-1]}\n\n" \
              f"\n Posterior Results:\n\n"\
              f"\n Accuracy (Posterior): {Accuracy_Posterior[-1]: 0.3f}\n" \
              f"\n NMI (Posterior): {NMI_Posterior[-1]: 0.3f}\n" \
              f"\n ARI (Posterior): {ARI_Posterior[-1]: 0.3f}\n" \
              f"\n Average Accuracy (Posterior): {overall_avg_accuracy_posterior}\n"\
              f"\n STD Accuracy (Posterior): {overall_std_accuracy_posterior}\n\n"\
              f"\n GMM Results:\n\n"\
              f"\n Accuracy (GMM): {Accuracy_GMM[-1]: 0.3f}\n"\
              f"\n NMI (GMM): {NMI_GMM[-1]: 0.3f}\n"\
              f"\n ARI (GMM): {ARI_GMM[-1]: 0.3f}\n"\
              f"\n Average Accuracy (GMM): {overall_avg_accuracy_gmm}\n"\
              f"\n STD Accuracy (GMM): {overall_std_accuracy_gmm}\n"\


file_path = f"Results_{Model_name}_{Dataset_name}.txt"
with open(file_path, "w") as text_file:
    text_file.write(text_to_save)

"""### **Maximum Accuracies (Posterior Probability)**"""

max_accuracy_indices_posterior = sorted(range(len(Accuracy_Posterior)), key=lambda i: Accuracy_Posterior[i], reverse=True)[:10]
max_accuracy_values_posterior = [Accuracy_Posterior[i] for i in max_accuracy_indices_posterior]
with open(f'Accuracy_Highest_Posterior_{Model_name}_{Dataset_name}.txt', 'w') as file:
    for idx2, accuracy2 in zip(max_accuracy_indices_posterior, max_accuracy_values_posterior):
        result_line = f"Index: {idx2}, \t Accuracy: {accuracy2:.4f}"
        file.write(result_line + '\n')
        print(result_line)

"""### **Maximum Accuracies (GMM)**"""

max_accuracy_indices = sorted(range(len(Accuracy_GMM)), key=lambda i: Accuracy_GMM[i], reverse=True)[:10]
max_accuracy_values = [Accuracy_GMM[i] for i in max_accuracy_indices]
with open(f'Accuracy_Highest_GMM_{Model_name}_{Dataset_name}.txt', 'w') as file:
    for idx, accuracy in zip(max_accuracy_indices, max_accuracy_values):
        result_line = f"Index: {idx}, \t Accuracy: {accuracy:.4f}"
        file.write(result_line + '\n')
        print(result_line)

"""### **True and Predicted Labels**"""

print(f"\n True Labels Frequency:\n\n {Counter(True_Labels[-1])}")
print(f"\n Posterior Labels Frequency:\n\n {Counter(Updated_Posterior_Labels[-1])}")
print(f"\n GMM Labels Frequency:\n{Counter(Updated_GMM_Labels[-1])}")

"""# **Graphs**

###**STD Accuracy**
"""

x=np.linspace(1, len(STD_Accuracy_GMM), len(STD_Accuracy_GMM))
plt.figure(figsize=(8,6))
plt.xlabel("Epochs")
plt.ylabel("SD (Accuracy)")
plt.plot(x, STD_Accuracy_GMM, color="blue", linestyle="solid", linewidth=1)
plt.grid()
plt.savefig(f"SD_Accuracy_Training_{Model_name}_{Dataset_name}.pdf", format="pdf", bbox_inches="tight")
plt.close()

"""###**Mean and Maximum Accuracy**"""

plt.figure(figsize=(8, 6))
x=np.linspace(1, len(Accuracy_GMM), len(Accuracy_GMM))
y=np.linspace(1, len(Mean_Accuracy_GMM), len(Mean_Accuracy_GMM))
plt.plot(x, Accuracy_GMM, label='Maximum Accuracy', color="blue", linestyle="solid", linewidth=1,  marker='o')
plt.plot(y, Mean_Accuracy_GMM, label='Mean_Accuracy', color="green", linestyle="solid", linewidth=1,  marker='x')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc="best")
plt.grid()
plt.savefig(f"Mean_Maximum_{Model_name}_{Dataset_name}.pdf", format="pdf", bbox_inches="tight")
plt.close()

"""###**Mean and Maximum Accuracy (After 10 epochs)**"""

list1=Accuracy_GMM
list2=Mean_Accuracy_GMM
indices = list(range(0, len(list1), 10))
accuracy1 = [list1[i] for i in indices]
accuracy2 = [list2[i] for i in indices]
plt.figure(figsize=(8, 6))
plt.plot(indices, accuracy1, label='Maximum Accuracy', color="blue", linestyle="solid", linewidth=1,  marker='o')
plt.plot(indices, accuracy2, label='Mean_Accuracy', color="green", linestyle="solid", linewidth=1,  marker='x')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc="best")
plt.grid()
plt.savefig(f"Mean_Maximum_{Model_name}_{Dataset_name}_10_epochs.pdf", format="pdf", bbox_inches="tight")
plt.close()

"""### **Training Losses and Test Losses**"""

plt.figure(figsize=(8, 6))
x=np.linspace(1, len(Training_Losses), len(Training_Losses))
y=np.linspace(1, len(Test_Loss), len(Test_Loss))
plt.plot(x, Training_Losses, label='Training Losses', color="blue", linestyle="solid", linewidth=1)
plt.plot(y, Test_Loss, label='Test Losses', color="green", linestyle="solid", linewidth=1)
plt.xlabel('Epochs')
plt.ylabel('Negative ELBO')
# plt.title('Training and Test Losses')
plt.legend(loc="best")
plt.grid()
plt.savefig(f"Training_Test_Loss_{Model_name}_{Dataset_name}.pdf", format="pdf", bbox_inches="tight")
plt.close()

"""### **Negative ELBO (Training Loss)**"""

x=np.linspace(1, len(Training_Losses), len(Training_Losses))
plt.figure(figsize=(8, 6))
plt.xlabel("Epochs")
plt.ylabel("Negative ELBO (Training Losses)")
plt.plot(x, Training_Losses, color="blue", linestyle="solid", linewidth=1)
plt.grid()
plt.savefig(f"Negative_ELBO_Training_Loss_{Model_name}_{Dataset_name}.pdf", format="pdf", bbox_inches="tight")
plt.close()

"""### **Negative ELBO (Test Loss)**"""

x=np.linspace(1, len(Test_Loss), len(Test_Loss))
plt.figure(figsize=(8, 6))
plt.xlabel("Epochs")
plt.ylabel("Negative ELBO (Test Loss)")
plt.plot(x, Test_Loss, color="blue", linestyle="solid", linewidth=1)
plt.grid()
plt.savefig(f"Negative_ELBO_Test_Loss_{Model_name}_{Dataset_name}.pdf", format="pdf", bbox_inches="tight")
plt.close()

"""### **Accuracy Plot**"""

Selected_Accuracy=Accuracy_Posterior[::GMM_interval]
plt.figure(figsize=(8, 6))
x=np.linspace(GMM_interval, len(Accuracy_GMM)*GMM_interval, len(Accuracy_GMM))
y=np.linspace(GMM_interval, len(Selected_Accuracy)*GMM_interval, len(Selected_Accuracy))
plt.plot(x, Accuracy_GMM, label='Accuracy (GMM)', color="blue", linestyle="solid", linewidth=1)
plt.plot(y, Selected_Accuracy, label='Accuracy (Posterior)', color="green", linestyle="solid", linewidth=1)
plt.xlabel('Epochs')
plt.ylabel('Clustering Accuracy')
plt.legend(loc="best")
plt.grid()
plt.savefig(f"Accuracy_{Model_name}_{Dataset_name}_GMM_Posterior.pdf", format="pdf", bbox_inches="tight")
plt.close()

"""### **Accuracy (Posterior)**"""

x=np.linspace(1, len(Accuracy_Posterior), len(Accuracy_Posterior))
plt.figure(figsize=(8, 6))
plt.xlabel("Epochs")
plt.ylabel('Clustering Accuracy')
plt.plot(x, Accuracy_Posterior, color="blue", linestyle="solid", linewidth=1)
plt.grid()
plt.savefig(f"Accuracy_{Model_name}_{Dataset_name}_Posterior.pdf", format="pdf", bbox_inches="tight")
plt.close()

"""### **Accuracy (GMM)**"""

x=np.linspace(GMM_interval, len(Accuracy_GMM)*GMM_interval, len(Accuracy_GMM))
plt.figure(figsize=(8, 6))
plt.xlabel("Epochs")
plt.ylabel('Clustering Accuracy')
plt.plot(x, Accuracy_GMM, color="blue", linestyle="solid", linewidth=1)
plt.grid()
plt.savefig(f"Accuracy_{Model_name}_{Dataset_name}_GMM.pdf", format="pdf", bbox_inches="tight")
plt.close()


"""### **NMI (GMM)**"""

x=np.linspace(GMM_interval, len(NMI_GMM)*GMM_interval, len(NMI_GMM))
plt.figure(figsize=(8, 6))
plt.xlabel("Epochs")
plt.ylabel('Normalized Mutual Information')
plt.plot(x, NMI_GMM, color="blue", linestyle="solid", linewidth=1)
plt.grid()
plt.savefig(f"NMI_GMM_{Model_name}_{Dataset_name}.pdf", format="pdf", bbox_inches="tight")
plt.close()

"""### **NMI (Posterior)**"""

x=np.linspace(1, len(NMI_Posterior), len(NMI_Posterior))
plt.figure(figsize=(8, 6))
plt.xlabel("Epochs")
plt.ylabel('Normalized Mutual Information')
plt.plot(x, NMI_Posterior, color="blue", linestyle="solid", linewidth=1)
plt.grid()
plt.savefig(f"NMI_Posterior_{Model_name}_{Dataset_name}.pdf", format="pdf", bbox_inches="tight")
plt.close()

"""### **ARI (GMM)**"""

x=np.linspace(GMM_interval, len(ARI_GMM)*GMM_interval, len(ARI_GMM))
plt.figure(figsize=(8, 6))
plt.xlabel("Epochs")
plt.ylabel('Adjusted Rand Index')
plt.plot(x, ARI_GMM, color="blue", linestyle="solid", linewidth=1)
plt.grid()
plt.savefig(f"ARI_GMM_{Model_name}_{Dataset_name}.pdf", format="pdf", bbox_inches="tight")
plt.close()


"""### **ARI (Posterior)**"""

x=np.linspace(1, len(ARI_Posterior), len(ARI_Posterior))
plt.figure(figsize=(8, 6))
plt.xlabel("Epochs")
plt.ylabel('Adjusted Rand Index')
plt.plot(x, ARI_Posterior, color="blue", linestyle="solid", linewidth=1)
plt.grid()
plt.savefig(f"ARI_Posterior_{Model_name}_{Dataset_name}.pdf", format="pdf", bbox_inches="tight")
plt.close()

"""# **Confusion Matrix and t-SNE**

### **Confusion Matrix (GMM)**
"""

conf_matrix = confusion_matrix(True_Labels[-1], Updated_GMM_Labels[-1])
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
# plt.title("Confusion Matrix")
plt.savefig(f"Confusion_Matrix_{Model_name}_{Dataset_name}_GMM.pdf", format="pdf", bbox_inches="tight")
plt.close()

"""### **Confusion Matrix (Posterior)**"""

conf_matrix = confusion_matrix(True_Labels[-1], Updated_Posterior_Labels[-1])
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
# plt.title("Confusion Matrix")
plt.savefig(f"Confusion_Matrix_{Model_name}_{Dataset_name}_Posterior.pdf", format="pdf", bbox_inches="tight")
plt.close()

"""### **Result: Posterior Probabilities (Neural Network)**"""

posterior_probabilities_nn=Posterior_NN[-1].cpu().numpy()
predicted_labels_nn = Updated_Posterior_Labels[-1]
true_labels = True_Labels[-1]
num_cols = posterior_probabilities_nn.shape[1]
column_labels = [f'c_{i}_nn' for i in range(num_cols)]
df_nn = pd.DataFrame({column_labels[i]: np.round(posterior_probabilities_nn[:, i], 2) for i in range(num_cols)})
posterior_columns_nn = [col for col in df_nn.columns if col.startswith('c_')]
max_values = df_nn[posterior_columns_nn].max(axis=1)
second_max_values = df_nn[posterior_columns_nn].apply(lambda row: sorted(row)[-2], axis=1)
df_nn['difference'] = max_values - second_max_values
df_nn['y_hat_nn']= predicted_labels_nn
df_nn['True_y']= true_labels
df_nn['Correct_nn'] = df_nn['y_hat_nn'] == df_nn['True_y']
df_nn_styled = df_nn.style.set_properties(**{'border-bottom': '1px solid black'})
df_nn.to_csv(f'Posterior_Probability_NN_{Model_name}_{Dataset_name}.csv', index=False)

"""### **Result: Posterior Probabilities (EM algorithm)**"""

posterior_probabilities_em=Posterior_EM[-1]
predicted_labels_gmm = Updated_GMM_Labels[-1]
true_labels = True_Labels[-1]
num_cols = posterior_probabilities_em.shape[1]
column_labels = [f'c_{i}_em' for i in range(num_cols)]
df_em = pd.DataFrame({column_labels[i]: np.round(posterior_probabilities_em[:, i], 2) for i in range(num_cols)})
posterior_columns_em = [col for col in df_em.columns if col.startswith('c_')]
max_values = df_em[posterior_columns_em].max(axis=1)
second_max_values = df_em[posterior_columns_em].apply(lambda row: sorted(row)[-2], axis=1)
df_em['difference'] = max_values - second_max_values
df_em['y_hat_em']= predicted_labels_gmm
df_em['True_y']= true_labels
df_em['Correct_gmm'] = df_em['y_hat_em'] == df_em['True_y']
df_em_styled = df_em.style.set_properties(**{'border-bottom': '1px solid black'})
df_em.to_csv(f'Posterior_Probability_GMM_{Model_name}_{Dataset_name}.csv', index=False)

"""### **t-SNE Visualization (GMM)**"""

latent_space_recoded=torch.tensor(Latent_embeddings_best[-1], dtype=torch.float32)
# tsne = TSNE(n_components=2, perplexity=5, random_state=random_seed)
tsne = TSNE(n_components=2, random_state=random_seed)
latent_tsne = tsne.fit_transform(latent_space_recoded)
plt.figure(figsize=(10, 10))
cmap = plt.cm.get_cmap('rainbow', len(np.unique(Updated_GMM_Labels[-1])))

scatter = plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], c=Updated_GMM_Labels[-1], cmap='rainbow')
plt.colorbar(scatter)
# plt.title('Visualization of Latent Space with Clusters')
plt.xlabel('Latent Embeddings (1)')
plt.ylabel('Latent Embeddings (2)')
# plt.grid(True)
# Create legend handles for each cluster
legend_handles = [mpatches.Patch(color=cmap(i), label=f'Cluster {i}') for i in np.unique(Updated_GMM_Labels[-1])]
plt.legend(handles=legend_handles, loc='best')

plt.savefig(f"t-SNE_Visualization_{Model_name}_{Dataset_name}_GMM.pdf", format="pdf", bbox_inches="tight")
plt.close()

"""### **t-SNE Visualization (Posterior)**"""

latent_space_recoded=torch.tensor(Latent_embeddings_best[-1], dtype=torch.float32)
# tsne = TSNE(n_components=2, perplexity=5, random_state=random_seed)
tsne = TSNE(n_components=2, random_state=random_seed)
latent_tsne = tsne.fit_transform(latent_space_recoded)
plt.figure(figsize=(10, 10))
cmap = plt.cm.get_cmap('rainbow', len(np.unique(Updated_Posterior_Labels[-1])))
scatter = plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], c=Updated_Posterior_Labels[-1], cmap='rainbow')
plt.colorbar(scatter)
# plt.title('Visualization of Latent Space with Clusters')
plt.xlabel('Latent Embeddings (1)')
plt.ylabel('Latent Embeddings (2)')
# plt.grid(True)
legend_handles = [mpatches.Patch(color=cmap(i), label=f'Cluster {i}') for i in np.unique(Updated_Posterior_Labels[-1])]
plt.legend(handles=legend_handles, loc='best')
plt.savefig(f"t-SNE_Visualization_{Model_name}_{Dataset_name}_Posterior.pdf", format="pdf", bbox_inches="tight")
plt.close()

"""## **Compare Reconstructions and Original Images**

### **Original Images**
"""

with torch.no_grad():
  print("Original Image")
  test_images=test_features.view(test_features.size(0), 1, pixel_size[0], pixel_size[1])
  test_images=test_images.cpu()
  test_images=test_images.clamp(0,1)
  test_images=test_images[:50]
  test_images=make_grid(test_images, 10, 5)
  test_images=test_images.numpy()
  test_images=np.transpose(test_images, (1, 2, 0))
  plt.imshow(test_images)
  plt.axis('off')
  plt.savefig(f'Original_Images_{Model_name}_{Dataset_name}.png')
  plt.close()

"""### **Reconstructed Images**"""

rows, cols = 5, 10
for k in range(num_components):
    decoder_mu_k = decoder_mu_test[:, k, :]
    with torch.no_grad():
        reconstructions = decoder_mu_k.view(decoder_mu_k.size(0), 1, pixel_size[0], pixel_size[1])
        reconstructions = reconstructions.cpu()
        reconstructions = reconstructions.clamp(0, 1)
        reconstructions = reconstructions[:rows * cols]
        reconstructions = make_grid(reconstructions, nrow=cols)
        reconstructions = reconstructions.numpy()
        reconstructions = np.transpose(reconstructions, (1, 2, 0))
        plt.imshow(reconstructions)
        plt.title(f"Reconstructed Images for k={k}")
        plt.axis('off')
        plt.savefig(f'Reconstructed_Images_{Model_name}_{Dataset_name}_k_{k}.png')
        plt.close()

"""### **Reconstructed Images (Best K)**"""

with torch.no_grad():
  print("Reconstructed Image")
  best_k=torch.argmax(q_y_i_k_test, dim=1)
  # best_k=torch.argmax(likelihood_test, dim=1)
  reconstructions=decoder_mu_test[torch.arange(decoder_mu_test.size(0)), best_k]
  reconstructions=reconstructions.view(reconstructions.size(0), 1, pixel_size[0], pixel_size[1])
  reconstructions=reconstructions.cpu()
  reconstructions=reconstructions.clamp(0,1)
  reconstructions= reconstructions[:50]
  reconstructions=make_grid(reconstructions, 10, 5)
  reconstructions=reconstructions.numpy()
  reconstructions=np.transpose(reconstructions, (1, 2, 0))
  plt.imshow(reconstructions)
  plt.axis('off')
  plt.savefig(f'Reconstructed_Images_{Model_name}_{Dataset_name}.png')
  plt.close()

"""### **Data Generation (Sampling Latent Space)**"""

torch.manual_seed(random_seed)
model.eval()
with torch.no_grad():
  print("Generated Image")
  z=torch.randn(100, latent_dim, device=device)
  new_image=model.decoder(z)
  new_image=new_image.view(new_image.size(0), 1, pixel_size[0], pixel_size[1])
  new_image=new_image.cpu()
  new_image=new_image.clamp(0, 1)
  new_image=new_image[: 100]
  new_image=make_grid(new_image, 10, 10)
  new_image=new_image.numpy()
  new_image=np.transpose(new_image,(1, 2, 0))
  plt.imshow(new_image)
  plt.axis('off')
  plt.savefig(f'Generated_Image_{Model_name}_{Dataset_name}_Std_1.png')
  plt.close()

torch.manual_seed(random_seed)
model.eval()
with torch.no_grad():
  print("Generated Image")
  z=torch.normal(0, 0.5, size=(100, latent_dim), device=device)
  new_image=model.decoder(z)
  new_image=new_image.view(new_image.size(0), 1, pixel_size[0], pixel_size[1])
  new_image=new_image.cpu()
  new_image=new_image.clamp(0, 1)
  new_image=new_image[: 100]
  new_image=make_grid(new_image, 10, 10)
  new_image=new_image.numpy()
  new_image=np.transpose(new_image,(1, 2, 0))
  plt.imshow(new_image)
  plt.axis('off')
  plt.savefig(f'Generated_Image_{Model_name}_{Dataset_name}_Std_0.5.png')
  plt.close()

torch.manual_seed(random_seed)
model.eval()
with torch.no_grad():
  print("Generated Image")
  z=torch.normal(0, 1.5, size=(100,latent_dim), device=device)
  new_image=model.decoder(z)
  new_image=new_image.view(new_image.size(0), 1, pixel_size[0], pixel_size[1])
  new_image=new_image.cpu()
  new_image=new_image.clamp(0, 1)
  new_image=new_image[: 100]
  new_image=make_grid(new_image, 10, 10)
  new_image=new_image.numpy()
  new_image=np.transpose(new_image,(1, 2, 0))
  plt.imshow(new_image)
  plt.axis('off')
  plt.savefig(f'Generated_Image_{Model_name}_{Dataset_name}_Std_1.5.png')
  plt.close()

torch.manual_seed(random_seed)
model.eval()
with torch.no_grad():
  print("Generated Image")
  z=torch.normal(-0.5, 1, size=(100,latent_dim), device=device)
  new_image=model.decoder(z)
  new_image=new_image.view(new_image.size(0), 1, pixel_size[0], pixel_size[1])
  new_image=new_image.cpu()
  new_image=new_image.clamp(0, 1)
  new_image=new_image[: 100]
  new_image=make_grid(new_image, 10, 10)
  new_image=new_image.numpy()
  new_image=np.transpose(new_image,(1, 2, 0))
  plt.imshow(new_image)
  plt.axis('off')
  plt.savefig(f'Generated_Image_{Model_name}_{Dataset_name}_Mean_Negative_0.5.png')
  plt.close()

torch.manual_seed(random_seed)
model.eval()
with torch.no_grad():
  print("Generated Image")
  mu = param_list[-3]
  logvar = param_list[-2]
  sd = torch.exp(0.5 * logvar)
  z = torch.normal(mu.repeat(10, 1), sd.repeat(10, 1)).to(device=device)
  # z=torch.normal(-0.5, 1, size=(100,latent_dim), device=device)
  new_image=model.decoder(z)
  new_image=new_image.view(new_image.size(0), 1, pixel_size[0], pixel_size[1])
  new_image=new_image.cpu()
  new_image=new_image.clamp(0, 1)
  new_image=new_image[: 100]
  new_image=make_grid(new_image, 10, 10)
  new_image=new_image.numpy()
  new_image=np.transpose(new_image,(1, 2, 0))
  plt.imshow(new_image)
  plt.axis('off')
  plt.savefig(f'Generated_Image_{Model_name}_{Dataset_name}_{random_seed}.png')
  plt.close()

torch.manual_seed(random_seed+2)
model.eval()
with torch.no_grad():
  print("Generated Image")
  mu = param_list[-3]
  logvar = param_list[-2]
  sd = torch.exp(0.5 * logvar)
  z = torch.normal(mu.repeat(10, 1), sd.repeat(10, 1)).to(device=device)
  # z=torch.normal(-0.5, 1, size=(100,latent_dim), device=device)
  new_image = model.decoder(z)
  new_image = new_image.view(new_image.size(0), 1, pixel_size[0], pixel_size[1])
  new_image = new_image.cpu()
  new_image = new_image.clamp(0, 1)
  new_image = new_image[: 100]
  new_image = make_grid(new_image, 10, 10)
  new_image = new_image.numpy()
  new_image = np.transpose(new_image, (1, 2, 0))
  plt.imshow(new_image)
  plt.axis('off')
  plt.savefig(f'Generated_Image_{Model_name}_{Dataset_name}_{random_seed+2}.png')
  plt.close()

torch.manual_seed(random_seed+4)
model.eval()
with torch.no_grad():
  print("Generated Image")
  mu = param_list[-3]
  logvar = param_list[-2]
  sd = torch.exp(0.5 * logvar)
  z = torch.normal(mu.repeat(10, 1), sd.repeat(10, 1)).to(device=device)
  # z=torch.normal(-0.5, 1, size=(100,latent_dim), device=device)
  new_image = model.decoder(z)
  new_image = new_image.view(new_image.size(0), 1, pixel_size[0], pixel_size[1])
  new_image = new_image.cpu()
  new_image = new_image.clamp(0, 1)
  new_image = new_image[: 100]
  new_image = make_grid(new_image, 10, 10)
  new_image = new_image.numpy()
  new_image = np.transpose(new_image, (1, 2, 0))
  plt.imshow(new_image)
  plt.axis('off')
  plt.savefig(f'Generated_Image_{Model_name}_{Dataset_name}_Updated_{random_seed+4}.png')
  plt.close()


"""### **Generated Image (GMM)**"""

torch.manual_seed(random_seed+6)
model.eval()
with torch.no_grad():
  print("Generated Image")
  gmm_mu = torch.tensor(gmm_mean[-1])
  gmm_covar = torch.tensor(gmm_covariance[-1])
  n_samples_per_component = 10
  all_samples = []
  for i in range(gmm_mu.shape[0]):
    dist = torch.distributions.MultivariateNormal(loc=gmm_mu[i], covariance_matrix=gmm_covar[i])
    component_samples = dist.sample(torch.Size([n_samples_per_component]))
    all_samples.append(component_samples)
  all_samples = torch.cat(all_samples, dim=0)
  z = all_samples.to(device=device, dtype=torch.float32)
  new_image = model.decoder(z)
  new_image = new_image.view(new_image.size(0), 1, pixel_size[0], pixel_size[1])
  new_image = new_image.cpu()
  new_image = new_image.clamp(0, 1)
  new_image = new_image[: 100]
  new_image = make_grid(new_image, 10, 10)
  new_image = new_image.numpy()
  new_image = np.transpose(new_image, (1, 2, 0))
  plt.imshow(new_image)
  plt.axis('off')
  plt.savefig(f'Generated_Image_{Model_name}_{Dataset_name}_{random_seed+6}_GMM.png')
  plt.close()



torch.manual_seed(random_seed+8)
model.eval()
with torch.no_grad():
  print("Generated Image")
  gmm_mu = torch.tensor(gmm_mean[-1])
  gmm_covar = torch.tensor(gmm_covariance[-1])
  n_samples_per_component = 10
  all_samples = []
  for i in range(gmm_mu.shape[0]):
    dist = torch.distributions.MultivariateNormal(loc=gmm_mu[i], covariance_matrix=gmm_covar[i])
    component_samples = dist.sample(torch.Size([n_samples_per_component]))
    all_samples.append(component_samples)
  all_samples = torch.cat(all_samples, dim=0)
  z = all_samples.to(device=device, dtype=torch.float32)
  new_image = model.decoder(z)
  new_image = new_image.view(new_image.size(0), 1, pixel_size[0], pixel_size[1])
  new_image = new_image.cpu()
  new_image = new_image.clamp(0, 1)
  new_image = new_image[: 100]
  new_image = make_grid(new_image, 10, 10)
  new_image = new_image.numpy()
  new_image = np.transpose(new_image, (1, 2, 0))
  plt.imshow(new_image)
  plt.axis('off')
  plt.savefig(f'Generated_Image_{Model_name}_{Dataset_name}_{random_seed+8}_GMM.png')
  plt.close()


torch.manual_seed(random_seed+6)
model.eval()
with torch.no_grad():
  print("Generated Image")
  gmm_mu = torch.tensor(gmm_mean[-1])
  gmm_covar = torch.tensor(gmm_covariance[-1])
  n_samples_per_component = 10
  all_samples = []
  for i in range(gmm_mu.shape[0]):
    mean = gmm_mu[i]
    covar_diag = torch.diag(gmm_covar[i])
    covar_matrix = torch.diag_embed(covar_diag)
    dist = torch.distributions.MultivariateNormal(loc=mean, covariance_matrix=covar_matrix)
    component_samples = dist.sample(torch.Size([n_samples_per_component]))
    all_samples.append(component_samples)
  all_samples = torch.cat(all_samples, dim=0)
  z = all_samples.to(device=device, dtype=torch.float32)
  new_image = model.decoder(z)
  new_image = new_image.view(new_image.size(0), 1, pixel_size[0], pixel_size[1])
  new_image = new_image.cpu()
  new_image = new_image.clamp(0, 1)
  new_image = new_image[: 100]
  new_image = make_grid(new_image, 10, 10)
  new_image = new_image.numpy()
  new_image = np.transpose(new_image, (1, 2, 0))
  plt.imshow(new_image)
  plt.axis('off')
  plt.savefig(f'Generated_Image_{Model_name}_{Dataset_name}_{random_seed+6}_GMM_diag.png')
  plt.close()

torch.manual_seed(random_seed+8)
model.eval()
with torch.no_grad():
  print("Generated Image")
  gmm_mu = torch.tensor(gmm_mean[-1])
  gmm_covar = torch.tensor(gmm_covariance[-1])
  n_samples_per_component = 10
  all_samples = []
  for i in range(gmm_mu.shape[0]):
    mean = gmm_mu[i]
    covar_diag = torch.diag(gmm_covar[i])
    covar_matrix = torch.diag_embed(covar_diag)
    dist = torch.distributions.MultivariateNormal(loc=mean, covariance_matrix=covar_matrix)
    component_samples = dist.sample(torch.Size([n_samples_per_component]))
    all_samples.append(component_samples)
  all_samples = torch.cat(all_samples, dim=0)
  z = all_samples.to(device=device, dtype=torch.float32)
  new_image = model.decoder(z)
  new_image = new_image.view(new_image.size(0), 1, pixel_size[0], pixel_size[1])
  new_image = new_image.cpu()
  new_image = new_image.clamp(0, 1)
  new_image = new_image[: 100]
  new_image = make_grid(new_image, 10, 10)
  new_image = new_image.numpy()
  new_image = np.transpose(new_image, (1, 2, 0))
  plt.imshow(new_image)
  plt.axis('off')
  plt.savefig(f'Generated_Image_{Model_name}_{Dataset_name}_{random_seed+8}_GMM_diag.png')
  plt.close()

"""## **PCA**

### **Visualizations**
"""

latent_embeddings = np.array(Latent_embeddings_best[-1])
pca = PCA(n_components=2, random_state=random_seed)
latent_pca = pca.fit_transform(latent_embeddings)
plt.figure(figsize=(10, 10))
cmap = plt.cm.get_cmap('rainbow', len(np.unique(Updated_GMM_Labels[-1])))
scatter = plt.scatter(latent_pca[:, 0], latent_pca[:, 1], c=Updated_GMM_Labels[-1], cmap='rainbow')
plt.colorbar(scatter)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
legend_handles = [mpatches.Patch(color=cmap(i), label=f'Cluster {i}') for i in np.unique(Updated_GMM_Labels[-1])]
plt.legend(handles=legend_handles, loc='best')
plt.savefig(f"PCA_Visualization_{Model_name}_{Dataset_name}.pdf", format="pdf", bbox_inches="tight")
plt.close()

# Get summary measures
mean = np.mean(latent_pca, axis=0)
variance = np.var(latent_pca, axis=0)
min_vals = np.min(latent_pca, axis=0)
max_vals = np.max(latent_pca, axis=0)
principal_axes = pca.components_
explained_variance = pca.explained_variance_
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_explained_variance = np.cumsum(explained_variance_ratio)
file_path = "pca_summary_measures.txt"
with open(file_path, "w") as file:
    file.write("Summary Measures:\n")
    file.write(f"Mean: {mean}\n")
    file.write(f"Variance: {variance}\n")
    file.write(f"Minimum: {min_vals}\n")
    file.write(f"Maximum: {max_vals}\n")
    file.write(f"Cumulative Explained Variance: {cumulative_explained_variance}\n")
    for i in range(len(principal_axes)):
        file.write(f"Principal Component {i + 1}:\n")
        file.write(f"Principal Axis (Loading):\n{principal_axes[i]}\n")
        file.write(f"Variance Explained: {explained_variance[i]:.2f}\n")
        file.write(f"Explained Variance Ratio: {explained_variance_ratio[i]:.2f}\n\n")

"""### **Generated Image (PCA)**"""

latent_space_original = pca.inverse_transform(latent_pca)
latent_space_original=torch.tensor(latent_space_original).to(device=device)
new_image = model.decoder(latent_space_original)
new_image = new_image.view(new_image.size(0), 1, pixel_size[0], pixel_size[1])
new_image = new_image.cpu()
new_image = new_image.clamp(0, 1)
new_image = new_image[: 100]
new_image = make_grid(new_image, 10, 10)
new_image = new_image.numpy()
new_image = np.transpose(new_image, (1, 2, 0))
plt.imshow(new_image)
plt.axis('off')
plt.savefig(f'Generated_Image_{Model_name}_{Dataset_name}_{random_seed+8}_PCA_Unmatched.png')
plt.close()

###############################################################################

predicted_labels = Updated_GMM_Labels[-1]
true_labels = True_Labels[-1]
selected_embeddings = []
for k in range(num_components):
  matching_index = np.where((true_labels == k) & (predicted_labels == k))[0]
  selected_latent=latent_space_original[matching_index]
  selected_latent= selected_latent[:10]
  selected_embeddings.append(selected_latent)

sorted_embeddings = torch.cat(selected_embeddings, dim=0)
new_image = model.decoder(sorted_embeddings)
new_image = new_image.view(new_image.size(0), 1, pixel_size[0], pixel_size[1])
new_image = new_image.cpu()
new_image = new_image.clamp(0, 1)
new_image = new_image[: 100]
new_image = make_grid(new_image, 10, 10)
new_image = new_image.numpy()
new_image = np.transpose(new_image, (1, 2, 0))
plt.imshow(new_image)
plt.axis('off')
plt.savefig(f'Generated_Image_{Model_name}_{Dataset_name}_{random_seed+8}_PCA_Matched.png')
plt.close()

###############################################################################

matched_indices = np.where(predicted_labels == true_labels)[0]
sorted_embeddings = latent_space_original[matched_indices]
new_image = model.decoder(latent_space_original)
new_image = new_image.view(new_image.size(0), 1, pixel_size[0], pixel_size[1])
new_image = new_image.cpu()
new_image = new_image.clamp(0, 1)
new_image = new_image[: 100]
new_image = make_grid(new_image, 10, 10)
new_image = new_image.numpy()
new_image = np.transpose(new_image, (1, 2, 0))
plt.imshow(new_image)
plt.axis('off')
plt.savefig(f'Generated_Image_{Model_name}_{Dataset_name}_{random_seed+8}_PCA_Matched_2.png')
plt.close()

###########################################################################################################
torch.manual_seed(random_seed)
model.eval()
with torch.no_grad():
  print("Generated Image")
  mu = param_list[-3]
  logvar = param_list[-2]
  var= torch.exp(logvar)
  covar = torch.diag_embed(var)
  n_samples_per_component = 1000
  n_smallest_distances = 10
  all_smallest_samples = []
  for i in range(mu.shape[0]):
    dist = torch.distributions.MultivariateNormal(loc=mu[i], covariance_matrix=covar[i])
    component_samples = dist.sample(torch.Size([n_samples_per_component]))
    component_distances = []
    for sample in component_samples:
      sample=sample.reshape(1,-1)
      distance = MD(sample, mu[i], covar[i])  # Compute distance for each sample
      component_distances.append(distance.item())
    smallest_indices = np.argsort(component_distances)[:n_smallest_distances]
    smallest_samples = component_samples[smallest_indices]
    all_smallest_samples.append(smallest_samples)
  all_smallest_samples = torch.cat(all_smallest_samples, dim=0)
  z = all_smallest_samples.to(device=device, dtype=torch.float32)
  new_image = model.decoder(z)
  new_image = new_image.view(new_image.size(0), 1, pixel_size[0], pixel_size[1])
  new_image = new_image.cpu()
  new_image = new_image.clamp(0, 1)
  new_image = new_image[: 100]
  new_image = make_grid(new_image, 10, 10)
  new_image = new_image.numpy()
  new_image = np.transpose(new_image, (1, 2, 0))
  plt.imshow(new_image)
  plt.axis('off')
  plt.savefig(f'Generated_Image_{Model_name}_{random_seed+6}_GMM_MD_Prior.png')
  plt.close()

###########################################################################################################
torch.manual_seed(random_seed+6)
model.eval()
with torch.no_grad():
  print("Generated Image")
  gmm_mu = torch.tensor(gmm_mean[-1])
  gmm_covar = torch.tensor(gmm_covariance[-1])
  n_samples_per_component = 1000
  n_smallest_distances = 10
  all_smallest_samples = []
  for i in range(gmm_mu.shape[0]):
    dist = torch.distributions.MultivariateNormal(loc=gmm_mu[i], covariance_matrix=gmm_covar[i])
    component_samples = dist.sample(torch.Size([n_samples_per_component]))
    component_distances = []
    for sample in component_samples:
      sample=sample.reshape(1,-1)
      distance = MD(sample, gmm_mu[i], gmm_covar[i])  # Compute distance for each sample
      component_distances.append(distance.item())
    smallest_indices = np.argsort(component_distances)[:n_smallest_distances]
    smallest_samples = component_samples[smallest_indices]
    all_smallest_samples.append(smallest_samples)
  all_smallest_samples = torch.cat(all_smallest_samples, dim=0)
  z = all_smallest_samples.to(device=device, dtype=torch.float32)
  new_image = model.decoder(z)
  new_image = new_image.view(new_image.size(0), 1, pixel_size[0], pixel_size[1])
  new_image = new_image.cpu()
  new_image = new_image.clamp(0, 1)
  new_image = new_image[: 100]
  new_image = make_grid(new_image, 10, 10)
  new_image = new_image.numpy()
  new_image = np.transpose(new_image, (1, 2, 0))
  plt.imshow(new_image)
  plt.axis('off')
  plt.savefig(f'Generated_Image_{Model_name}_{random_seed+6}_GMM_MD.png')
  plt.close()

###########################################################################################################

print(f"Experiment Completed on Model: {Model_name} and {Dataset_name} Dataset")
