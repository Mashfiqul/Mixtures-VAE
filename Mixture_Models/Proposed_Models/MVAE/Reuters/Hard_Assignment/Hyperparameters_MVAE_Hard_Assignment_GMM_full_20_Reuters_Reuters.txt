Dataset: Reuters
--------------------------------------------
Model Name: MVAE_Hard_Assignment_GMM_full_20_Reuters
==================================================================
MVAE_Hard_Assignment_GMM_full_20_Reuters Part:
==================================================================
Random Seed = 20
--------------------------------------------
Device = cuda:1
--------------------------------------------
Number of Epochs = 2000
--------------------------------------------
Learning Rate = 0.0001
--------------------------------------------
Step Size = 20
--------------------------------------------
Input Dimension = 2000
--------------------------------------------
Encoder Hidden Dimension 1 = 500
--------------------------------------------
Encoder Hidden Dimension 2 = 500
--------------------------------------------
Encoder Hidden Dimension 3 = 2000
--------------------------------------------
Latent Dimension = 5
--------------------------------------------
Decoder Hidden Layer 1 = 2000
--------------------------------------------
Decoder Hidden Layer 2 = 500
--------------------------------------------
Decoder Hidden Layer 3 = 500
--------------------------------------------
Output Dimension = 2000
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 100
--------------------------------------------
Batch Size (Test) = 100
--------------------------------------------
gamma = 0.9
==================================================================

GMM Part:
==================================================================
Number of Components= 4
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-20
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 2001
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[-2501.6489, -2487.4529, -2586.8081, -2492.3184],
        [-2779.9663, -2774.3066, -2785.9771, -2776.1426],
        [-2586.8760, -2624.4248, -2589.3000, -2587.1143],
        [-2577.7305, -2650.1328, -2563.7046, -2564.9917],
        [-2573.9507, -2613.1958, -2574.6082, -2779.5688],
        [-2788.9053, -2787.9663, -2785.8333, -2784.1553],
        [-2523.2993, -2603.4668, -2525.6724, -2525.0703],
        [-2564.4346, -2594.2085, -2564.4556, -2654.0564],
        [-2571.8806, -2585.4932, -2579.5522, -2681.9585],
        [-2511.2830, -2517.5635, -2514.2178, -2518.1230],
        [-2333.0688, -2407.4683, -2474.8623, -2816.0620],
        [-2546.1138, -2585.0688, -2558.1699, -2717.3054],
        [-2709.4731, -2718.2434, -2721.1338, -2706.8613],
        [-2709.6958, -2711.6282, -2760.3833, -2759.5945],
        [-2756.2556, -2763.7556, -2761.2432, -2810.1936],
        [-2473.4282, -2490.3594, -2507.9771, -2543.6885],
        [-2473.4097, -2497.1379, -2470.2271, -2845.1497],
        [-2383.9631, -2419.5908, -2775.3594, -2979.7676],
        [-2625.8037, -2712.5681, -2621.9106, -2620.9592],
        [-2139.5159, -2184.1150, -2142.7717, -2392.6548],
        [-2676.1125, -2677.6978, -2685.5303, -2678.3894],
        [-2165.6221, -2173.4058, -2403.5876, -2198.8315],
        [-2726.2349, -2731.1572, -2729.3164, -2724.4307],
        [-2671.5190, -2676.0476, -2671.7373, -2674.7637],
        [-2446.5239, -2466.8896, -2447.4648, -2795.7734],
        [-2010.3713, -2000.3821, -2170.9648, -2016.8615],
        [-2802.2266, -2681.6226, -2440.0742, -2439.8584],
        [-2706.8320, -2739.6460, -2709.0005, -2810.1401],
        [-2687.7676, -2693.3926, -2681.4771, -2685.6001],
        [-2424.4578, -2466.5103, -2499.6060, -2735.1479],
        [-2620.0225, -2628.6494, -2633.8486, -2622.8743],
        [-2754.4399, -2754.0146, -2752.7056, -2812.4111],
        [-2601.9531, -2700.8774, -2603.0176, -2611.4390],
        [-2570.5833, -2595.3020, -2568.0513, -2567.4172],
        [-2740.8650, -2741.5981, -2739.2004, -2742.7559],
        [-2650.4094, -2678.3408, -2655.7119, -2649.6738],
        [-2335.1055, -2276.9780, -2361.2539, -2272.6919],
        [-2617.9756, -2627.9087, -2622.0830, -2620.5886],
        [-2609.7285, -2653.1826, -2607.9231, -2605.0996],
        [-2589.1714, -2657.7834, -2592.7200, -2591.9663],
        [-2728.9639, -2729.1465, -2722.8423, -2722.4609],
        [-2615.9644, -2636.5630, -2624.8914, -2627.0740],
        [-2599.3425, -2607.2498, -2599.6511, -2596.2231],
        [-2888.1880, -2873.3755, -2841.2173, -2787.1270],
        [-2766.1685, -2756.9580, -2759.0640, -2765.1941],
        [-2756.8613, -2748.6038, -2748.4219, -2758.5425],
        [-2680.5098, -2677.5063, -2676.7703, -2689.2095],
        [-2368.5935, -2395.1050, -2441.8125, -2666.9048],
        [-2641.8442, -2644.6665, -2640.1348, -2643.8560],
        [-2635.4929, -2634.1624, -2631.0620, -2640.6345],
        [-2731.1084, -2739.0354, -2738.3596, -2735.1248],
        [-2863.1145, -2775.8569, -2436.1187, -2431.3789],
        [-2410.4927, -2439.4409, -2343.9644, -2429.9961],
        [-2547.3535, -2548.4360, -2586.7063, -2577.8672],
        [-2431.9509, -2479.5376, -2467.2522, -2898.0693],
        [-2686.2769, -2772.4792, -2673.6675, -2676.5859],
        [-2620.1016, -2621.9028, -2628.2893, -2618.1570],
        [-2923.5957, -2756.2041, -2481.9980, -2220.5098],
        [-2647.4690, -2658.6943, -2646.2007, -2907.6060],
        [-2627.8784, -2643.3013, -2625.5889, -2629.7241],
        [-2707.2305, -2738.1084, -2706.8105, -2710.6174],
        [-2122.3472, -2195.4219, -2865.4946, -2916.8423],
        [-2703.5012, -2724.0928, -2650.2319, -2651.8416],
        [-2518.3579, -2574.6531, -2509.9507, -2522.4932],
        [-2615.3401, -2624.4661, -2622.7930, -2869.8582],
        [-2553.4858, -2546.4983, -2565.1294, -2545.6567],
        [-2463.3298, -2453.0234, -2424.5000, -2462.7627],
        [-2689.7939, -2688.9307, -2709.2188, -2687.7559],
        [-2666.3682, -2667.1885, -2668.9390, -2671.6709],
        [-2627.2361, -2633.1228, -2629.3154, -2802.2002],
        [-3068.6841, -3012.1382, -2499.8965, -2499.0105],
        [-2484.1519, -2504.6548, -2518.6924, -2720.6772],
        [-2481.2812, -2489.8477, -2601.7627, -2733.2056],
        [-2698.7202, -2699.6658, -2707.3105, -2705.1736],
        [-2721.9360, -2731.2371, -2720.4646, -2719.1724],
        [-2736.4487, -2766.0728, -2746.7520, -2735.4009],
        [-2343.7917, -2370.0315, -2792.7253, -2777.9912],
        [-2388.9531, -2388.7529, -2470.7329, -2425.0488],
        [-2723.0281, -2738.9609, -2722.5327, -2727.1633],
        [-2431.8130, -2462.0576, -2435.4133, -2732.0645],
        [-2409.2292, -2415.9785, -2532.5029, -2531.3696],
        [-2635.2566, -2632.4751, -2636.8174, -2630.8945],
        [-2442.2363, -2515.7292, -2448.1553, -2700.6628],
        [-2591.6436, -2593.7915, -2594.5347, -2603.1553],
        [-2374.8347, -2416.6882, -2377.7119, -2893.5723],
        [-2522.6523, -2543.9922, -2529.8237, -2802.3228],
        [-2904.0938, -2864.9824, -2544.3340, -2549.6790],
        [-2375.9062, -2369.0264, -2430.5933, -2388.7136],
        [-2155.1509, -2200.1487, -2675.1536, -2670.3730],
        [-2621.2607, -2664.8301, -2414.9353, -2418.7888],
        [-2564.4917, -2582.3149, -2476.9871, -2474.4731],
        [-2592.0537, -2648.4707, -2584.6709, -2588.3208],
        [-2576.4893, -2594.6328, -2571.9497, -2561.1011],
        [-2633.1497, -2670.7109, -2617.9680, -2620.4312],
        [-2681.5378, -2683.1143, -2687.3101, -2680.0845],
        [-2749.4521, -2759.0312, -2749.5542, -2754.3794],
        [-2569.8857, -2584.9268, -2569.2671, -2600.0188],
        [-2468.2957, -2530.3315, -2474.4160, -2485.0139],
        [-2426.9546, -2431.8706, -2463.4575, -2732.2061],
        [-2578.8169, -2608.9316, -2570.8110, -2571.0791]], device='cuda:1',
       grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([100, 4, 5]), scale: torch.Size([100, 4, 5]))
=============================================================================================
Decoder Loss (Test) = tensor([[-2811.3198, -2791.5642, -2804.4241, -2802.0220],
        [-2654.2971, -2648.5991, -2654.7817, -2655.2139],
        [-2372.9204, -2433.7593, -2591.7739, -2821.2446],
        [-2158.6760, -2209.1626, -2423.5889, -2599.8447],
        [-2749.9158, -2756.1494, -2750.7979, -2748.9919],
        [-2774.0081, -2768.1079, -2774.7329, -2771.4297],
        [-2554.9224, -2568.2891, -2553.3674, -2561.2244],
        [-2801.5527, -2728.5767, -2556.5410, -2552.7769],
        [-2528.3330, -2529.8708, -2537.6055, -2580.5718],
        [-2466.1885, -2458.2190, -2444.4189, -2520.9062],
        [-2692.3501, -2669.2664, -2551.6711, -2550.1147],
        [-2550.9404, -2597.0247, -2514.5273, -2868.6626],
        [-2809.6914, -2811.4153, -2808.1128, -2806.4951],
        [-2016.5696, -1980.5848, -2504.4199, -1987.0768],
        [-2728.3660, -2728.9067, -2730.9434, -2730.1890],
        [-2625.7856, -2617.7876, -2619.6655, -2619.7104],
        [-2765.9429, -2763.6392, -2764.2629, -2767.5186],
        [-2795.4561, -2791.9836, -2796.1914, -2796.2505],
        [-2622.3281, -2630.6626, -2616.6611, -2880.8530],
        [-2680.4082, -2671.9419, -2677.4568, -2677.9436],
        [-2395.1250, -2385.4727, -2467.7764, -2385.7629],
        [-2053.8726, -2035.8785, -2182.8569, -2088.3721],
        [-2600.3984, -2603.4531, -2601.5522, -2621.2188],
        [-2534.1382, -2532.3999, -2611.0884, -2534.1387],
        [-2410.1194, -2424.2310, -2412.2437, -2879.2400],
        [-2291.7168, -2337.9089, -2452.0098, -2642.4976],
        [-2463.2388, -2479.2251, -2461.8535, -2851.4380],
        [-2651.4434, -2646.8196, -2650.8794, -2652.6475],
        [-2763.4426, -2563.7185, -2733.8831, -2553.2817],
        [-2657.5557, -2656.3057, -2655.0461, -2698.0771],
        [-2760.7173, -2762.5361, -2756.3965, -2769.7668],
        [-2628.0715, -2604.1772, -2661.0073, -2623.0122],
        [-2378.0249, -2366.0481, -2488.6802, -2385.0718],
        [-2653.9502, -2670.6108, -2653.9463, -2652.9426],
        [-2611.0513, -2639.3003, -2610.7207, -2878.2080],
        [-2713.6411, -2723.1333, -2717.6401, -2718.6577],
        [-2605.8442, -2582.2585, -2629.3962, -2577.5107],
        [-2670.8713, -2670.1709, -2668.3936, -2665.9717],
        [-2674.0488, -2683.7036, -2653.9077, -2658.6006],
        [-2674.8833, -2671.7683, -2671.1519, -2672.5098],
        [-2534.2314, -2568.2002, -2662.6533, -2717.0811],
        [-2540.6294, -2549.3369, -2612.1011, -2678.5688],
        [-2662.8716, -2679.2490, -2665.8579, -2659.8066],
        [-2765.5435, -2765.3379, -2765.5166, -2811.3374],
        [-2661.0354, -2657.9585, -2674.7407, -2659.9102],
        [-2634.8481, -2623.9023, -2699.6533, -2628.2073],
        [-2697.0396, -2712.8267, -2698.3301, -2697.6409],
        [-2506.5598, -2504.5225, -2525.0999, -2529.9543],
        [-2280.2988, -2282.8613, -2421.7842, -2280.1304],
        [-2756.5396, -2759.7395, -2758.1182, -2756.6082],
        [-2437.1829, -2542.5569, -2440.1750, -2802.2114],
        [-2607.8054, -2620.9902, -2627.3176, -2621.1523],
        [-2639.4097, -2636.1709, -2630.5571, -2633.7837],
        [-2706.6716, -2712.6323, -2707.0664, -2795.2310],
        [-2695.5701, -2678.9653, -2683.9844, -2682.5000],
        [-2801.4285, -2799.7668, -2782.8735, -2781.9990],
        [-2682.0181, -2716.7795, -2663.4072, -2815.8076],
        [-2110.6006, -2242.5510, -2209.8286, -2443.9282],
        [-2390.2419, -2419.2205, -2586.9355, -2745.0156],
        [-2654.4546, -2642.3420, -2631.1199, -2628.6807],
        [-2745.9224, -2735.1575, -2765.2590, -2632.6255],
        [-2786.6335, -2791.8345, -2795.7966, -2789.6218],
        [-2630.6626, -2674.5767, -2631.7263, -2627.9199],
        [-2791.2153, -2787.1709, -2796.0176, -2770.2214],
        [-2762.4897, -2787.3079, -2757.4102, -2756.8853],
        [-2249.5342, -2250.0295, -2348.0486, -2665.4050],
        [-2685.5342, -2684.4990, -2701.2483, -2677.3169],
        [-2681.4355, -2711.9434, -2674.2856, -2665.0659],
        [-2506.4678, -2501.2524, -2505.8965, -2859.7119],
        [-2825.7878, -2817.0493, -2835.7874, -2823.7791],
        [-2504.6079, -2541.4700, -2505.0300, -2590.8730],
        [-2669.4302, -2666.1426, -2665.6155, -2666.5366],
        [-2644.3203, -2651.8506, -2644.4663, -2636.9541],
        [-2556.9644, -2567.2729, -2565.0977, -2556.0349],
        [-2678.4897, -2697.9019, -2678.4297, -2885.1924],
        [-2445.2661, -2458.2642, -2502.8464, -2614.2852],
        [-2637.4844, -2645.9646, -2636.8838, -2723.3972],
        [-2672.7539, -2677.2788, -2670.6357, -2791.8936],
        [-2594.7551, -2599.4517, -2595.3018, -2634.8740],
        [-2714.6135, -2703.8289, -2711.8540, -2713.7034],
        [-2387.3657, -2465.7949, -2387.2227, -2791.1738],
        [-2444.1277, -2442.9204, -2493.6519, -2439.1265],
        [-2690.5007, -2690.3752, -2724.9937, -2714.6968],
        [-2670.9019, -2679.0488, -2675.0977, -2901.9067],
        [-2684.3979, -2667.8306, -2685.9517, -2668.5630],
        [-2734.1475, -2755.5007, -2737.5713, -2731.6660],
        [-2405.5591, -2423.9888, -2334.4131, -2451.1299],
        [-2065.8423, -2115.0649, -2327.6851, -2571.8459],
        [-2831.9551, -2816.8049, -2836.3989, -2851.6482],
        [-2176.9563, -2173.5557, -2288.0679, -2178.5566],
        [-2268.8621, -2320.7583, -2658.0317, -2851.5581],
        [-2806.8149, -2807.8782, -2804.5225, -2804.5503],
        [-2673.6592, -2729.9688, -2621.6572, -2634.4731],
        [-2446.3960, -2490.7498, -2631.9336, -2929.2373],
        [-2341.5286, -2370.1497, -2608.5581, -2627.2241],
        [-2608.9080, -2610.5703, -2605.6858, -2603.8784],
        [-2665.1589, -2646.5881, -2576.1533, -2581.8071],
        [-2371.7156, -2397.5498, -2615.8696, -2651.3074],
        [-2687.6182, -2676.6123, -2690.0483, -2724.4656],
        [-2781.6990, -2769.5493, -2757.8108, -2765.6248]], device='cuda:1')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([100, 4, 5]), scale: torch.Size([100, 4, 5]))
=============================================================================================
